# まえがき
データ分析・応用において必須の道具である**数理モデル**について俯瞰的に解説した本．
データ分析はデータの生成過程をよく表現する数理モデルを構築し，そこから情報を引き出すこと．
データ分析の本質に加えて，広い視点でデータ分析を眺める．

- 数理モデルを使ったデータ分析で何が出来るか分からない
- 今自分が使っている数理モデルは適切なモデルなのか，他の可能性があるならどうやって探すのか
- 数理モデルの振る舞いや性質を理解して，より本質に迫ったデータ分析をしたい

ということに迫ることが出来る．各トピックを深追いせずに，データ分析を俯瞰的に勉強する．

---
---
---

# 第一部 数理モデルとは
数理モデルとは何か，通常のデータ分析とは何が違うのかについて解説する．
そして，それらの分析によって何が可能になるのかについて紹介する．

---
## 第1章 データ分析と数理モデル
データを分析するとは何か？
数理モデルを使うことが何を意見するのか？
常に得られたデータだけから有益な情報を取り出さなければならない．

### 1.1 データを分析するということ
#### 人間の認知限界とデータ分析
データ分析とは・・・  
対象から情報を取得して，それを分析することで，対象がどのようなメカニズムで動いているのかを客観的に理解すること

#### 対象をデータ生成システムとして捉える
- システム(system): データ分析の対象となるもの
- 観測(observation): 人間がシステムからデータを得ること
- 生成(generate): システムがデータを出すこと
システムから生成されるデータを観測し，その生成ルールを推測する

#### データ分析における限界
データ分析をすれば何でも分かるわけではない．
得られるデータはシステムの一部の状況を反映したものにすぎない．
データ分析を妨げる要因  
- データが足りない
- システムの一部分しか観測できない
- システムの現象が極めて複雑

#### 2つのアプローチ
- 要素還元論的アプローチ
    - システムを分解し，少ない種類の要素の集まりとして理解する
- 新しいアプローチ
    - 要素分解しないアプローチ．
    - 複雑なものは複雑なまま分析する．深層学習などのデータ駆動型(data-driven)の分析パラダイム


---
### 1.2 数理モデルの役割
#### データを眺める以上の分析が必要なら数理モデルの出番
記述統計量(descriptive statistics)や可視化も大事．しかしながら，それだけでは出来ないこともある．

- 現象のメカニズムを客観的な方法で明らかにする
- データから未来を予測する
- コンピュータに高度なデータ処理・データ生成をやらせる

これらを解決するために，数理モデル(mathematical model)を用いる． データを眺める以上の分析が必要なら数理モデルの出番
記述統計量(descriptive statistics)や可視化も大事．しかしながら，それだけでは出来ないこともある．

- 現象のメカニズムを客観的な方法で明らかにする
- データから未来を予測する
- コンピュータに高度なデータ処理・データ生成をやらせる

これらを解決するために，数理モデル(mathematical model)を用いる．  
数理モデル: 数学な手段を用いて記述された，対象のデータ生成ルールを模したもの．  
実際の分析対象を操作することはできないので，数理モデルを介して現象の理解を行う．

#### 数理モデルは「仮定」である

数理モデルによって得られた結論は，常に**使用したモデルという仮定の下**で，という条件付きになっている．

---

### 第1章のまとめ

- 数理モデルとは，数学的な手段を用いて記述された，**対象のデータ生成のルールを模した**もの
- データを眺めただけでは分からない**メカニズムの理解や，予測などの高度な分析・応用**が必要な時に数理モデルが活躍する
- 数理モデルによる分析は，モデルごとに**数理的な「仮定」を前提としている**ことに注意

---
---

## 第2章 数理モデルの構成要素・種類
数理モデルの種類・構成する要素について解説する．

---
### 2.1 変数・数理構造・パラメータ
#### 数理モデルを構成する要素

- 変数(variavle)
- 数理構造(mathematical structure)
- パラメータ(parameter)

#### まずは変数で表すことから

変数: システムの何らかの状態，性質，量などを数字やラベルで表したもの

量的変数と質的変数: 値の性質による分類

- 量的変数(quantitative variable): 足し算，引き算が出来る．身長，気温
    - 間隔尺度: 目盛りが等間隔になっているもの
    - 比例尺度: 原点があり，割り算も意味がある．身長
- 質的変数(qualitative variable): カテゴリ変数(categorical variable)．性別，順位．
    - 名義尺度: 他と分類できるもの．性別
    - 順序尺度: 順序に意味があるが，間隔には意味がない．順位

観測変数と潜在変数: 観測できるか否か

- 観測変数(observable variavle): 直接取得できるデータ．顧客の購入数．
- 潜在変数(latent variable): 取得できないデータ．購入理由

目的変数と説明変数: 説明するかされるか

- 目的変数(objective variable), 従属変数(dependent variavle): 計算される変数
- 説明変数(explanatory varibale), 独立変数(independent variabele): 計算に使う変数

#### 数理構造=「数理モデルの骨組み」

数理モデル: 着目する変数の間の関係性を，数学的に表現したもの．
適切な数理モデルを選ぶことが良い分析の要になる．

#### パラメータは数理モデルを「動かす」

パラメータ: 数理モデルをデータに合わせるための可動域を制御する．
パラメータの数が多くなると表現力が増すが，取り扱いが難しくなる．  
フィッティング:  数理モデルをデータに合わせること

例．体重と身長の線形回帰モデル
$$
W = aH + b
$$
において，全体を数理構造，特にa, bをパラメータと呼ぶ．

---
### 2.2 数理モデルと自然科学の基礎理論
#### 確立した数理モデルは基礎理論に

- ニュートンの運動方程式
$
m\frac{d^2r}{dt^2} = F
$

微分は何かの，何かに対する変化の割合を計算したものであり，非常に重要である．


#### 境界条件と計算の難易度

境界条件(boundary condition): 数理モデルを適用できる範囲．現実問題では取り扱いが難しい

---
### 2.3 理解志向型モデリングと応用志向型モデリン 'Source Han Code JPv
#### 目的によってモデリングは大きく異なる
理解志向型モデリング: データの生成メカニズムを理解する．  
どの要因が強く影響しているのか，なぜそのような現象がおこるのか

応用志向型モデリング: 手元にあるデータを元に未知のデータに対して予測・制御を行なったり，新しいデータを生成して利用する．
現象の理解よりも応用に実装した時の性能を重視する
ex.) 画像の判別，機械翻訳など．

#### モデルの複雑さと理解のしやすさ
モデルの複雑さ: 変数やパラメータの数が多い，数理構造に使われる関数が複雑．


---
### 2.4 理解志向型モデリング
#### 理解志向型モデリングの方法
基本的な考え方: データをうまく説明する数理モデルはデータの生成過程を捉えているからその数理モデルを調べることで理解につながる．大きな方針としては

- 数理構造から説明する方法
- 推定したパラメータの値から説明する方法
- 推定された潜在変数や内部表現を使って次の解析を行う方法
- 数理モデルのパラメータを変化させた状況をシミュレーションする方法

#### 数理構造から説明する方法
モデルがデータを説明できるなら，数理モデルを作る時に仮定した数理構造が正しかったという考え．  
ex)自然渋滞をモデル化した場合．

現象が説明されたとしても，モデルを構成する全ての要素が正しいわけではない．
仮定して良さそうな経験や観測事実からボトムアップ的に論理を構成する

#### 推定したパラメータの値から説明する方法
パラメータの係数から減少を説明する．
ただし，その情報量は数理モデルの質による

#### 推定された潜在変数や内部表現を使って次の解析を行う方法
多次元データを，一度潜在変数を持つ数理モデルで表現し，その潜在変数を使って低次元の表現に変換する．
人間の理解しづらい部分を数理構造でまとめるため，理解しやすくなる．

混合分布モデル(mixture model)や状態空間モデル(state space model)など．
ex) 脳活動の例．複数の領域を混合分布モデルを用いてそれぞれの潜在変数に割り当てることで，低次元データに変換する

#### 数理モデルのパラメータを変化させた状況をシミュレーションする
仮想的に色々な値を入れて振る舞いをシミュレーションすることで，対象の理解を深める．

ex) セルラーゼがセルロースを分解するモデルを組み立て，変数を変化させることで分解速度低下の原因を探る

---
### 2.5 応用志向型モデリング
#### 数理モデルのデータ生成能力を活用する
- 予測(predict): 数理モデルを作った時に使ったデータと異なるデータを入れて，予測する．
- 生成(generation): 数理モデルを作った時に使用したのと同じようなデータを出力すること．
    - 出力: 変数の一部を出力変数として，人間が取り出すこと．
    - 入力: 入力変数として人間が指定すること

#### 予測モデルの例1: 値の予測(回帰)
回帰(regression): 値を予測すること
データによくあったモデルは予測することができる．

- 内挿(interpolation): 観測データの間にある値を予測すること
- 外挿(extrapolation): データが存在していない領域について予測すること．精度が悪くなったり，正しく認識できないことがある．

#### 予測モデルの例2: 分類問題
分類(classification): ラベルやグループに分ける．

- 教師あり学習(supervised learning): ラベルがわかっている場合．
- 教師なし学習(unsupervised learning): ラベルが潜在変数となっている場合．にているデータをまとめる．クラスタリング．

#### 生成モデルを使った応用
生成モデルは学習データがどのように生成されているかそのものを学習する．
敵対的生成ネットワーク(generative adversarial networl: GAN)など．

---
### 2.6 数理モデルの限界と適用範囲
#### 「正しい」数理モデル?
数理モデルは万能ではない．あくまでも現象の近似であり，扱うのに都合が良いからであって正しいわけではない．
数理モデルを作るだけでなく，そのモデルを使って何をしたいかまで考える．

ピグマリオン症: 数理モデルと現実を混同すること．
個人的には数理モデルは仮想世界であり，実データもバイアスがかかったものでそれが全てを表しているわけではないということを留めておく必要があると思う．

#### 良い数理モデルも，いつも正しいとは限らない
データをよく記述する数理モデルを使って分析・応用する際には，「データが得られた時の状況と，説明しようとしている状況が同じ」という仮定がある．
用いたデータの範囲外にある時は，モデルの精度は保証されない．
モデルの適用範囲やリスクに対する頑強性について正しく評価を行う必要がある．

ex) 未曾有の大災害や異常気象など．

---
### 第2章のまとめ
- 数理モデルとは，注目する量を表した変数たちの関係性を数理的に表したもの
- 変数間の関係性は，モデルの骨組みである数理構造とその可動域を決めるパラメータで決まる
- 数理モデルは，目的に応じて理解志向型モデリングや応用志向型モデリングに分けられる
- **数理モデルは万能ではない．目的や適用範囲を謝ると，役に立たず，誤った結論を導く可能性もある**

---
---
---

# 第二部 基礎的な数理モデル
数理モデルにおいて重要な基礎概念である，関数のフィッティング，微分方程式，確率過程，統計解析について説明する．

---
## 第3章 少数の方程式によるモデル
変数間の関係性が少ない数の方程式で記述される数理モデルについて紹介する．

### 3.1 線形モデル

#### 変数間の関係を等式で表現する
数学的に扱いやすく，変数同士の関係や影響の度合いを分析することが可能

#### 線形モデル
線形モデル(linear model): 線型結合(加減・定数倍)で表すことができるモデル．

### 線形モデルを求めるには
パラメータの値をデータに最も当てはまるようにしたい

### 最小二乗法
最小二乗法(least squares method): 二乗誤差を最小にするパラメータを推定する．
詳細な計算は省くが，偏微分することで求まる．

多重共線性: 説明変数にうち，お互いに関与しあっていること．どっちのパラメータを重視して良いか分からなくなり，結果が不安定になる．


---
### 3.2 実験式・カーブフィッティング
#### BMIと数理モデル
$$
    体重[kg] = (定数) \times 身長^2 [m^2]
$$

#### 冪乗による特徴づけ
冪乗則(power law), スケーリング(scaling): ある変数の値が別の変数の値の何乗かに比例する．
この指数を決めているメカニズムを知ることで，本質的なことがわかる場合がある

### 対数プロットで冪乗則を探す
$$
    Y = cX^\alpha \\
    log{Y} = \alpha log{X} + log{a}
$$
対数グラフの傾きを見ることで，$\alpha$を決めることができる．
しかしあんがら，データの和也変数の範囲が不十分だと誤った結果を導くことが多い

### 実験式
実験式(experimental formula): データの振る舞いに合わせて，関数を設定する．
データの可視化を通して組み立て，データと式の誤差からパラメータを計算する．
これには多項式や指数関数，三角関数などが用いられることが多い．


### 3.3 最適化問題
#### 目的の量をコントロールする
最適化問題(optimization problem): 変数を適切に設定して，ある量を最小化・最大化すること．

目的関数(objective function): 最適化問題を行う関数．

#### 大域的最適と局所的最適
局所的最適解(local optimal solution): 周囲よりも小さい・大きい点．

大域的最適解(global optimal solution): 全体で最も小さい・大きい点．

最適化問題が難しくなると，大域最適解を求めるのは難しくなる．様々なアルゴリズムが開発され，計算されている．

#### パラメータ調整も最適化
数理モデルのパラメータを設定するのは，データとモデルの間の誤差を目的関数として最小化するという最適化問題．最適化問題

#### 最適化を難しくする要素
- 変数の制約条件が多い
- 変数が離散的
- グラフやネットワークに関する最適化が含まれている
- 組合せに関する問題が含まれている(組合せ最適化問題)
- 変数の数や，離散変数のとりうる値の場合の数が多い
- 問題に微分不可能な関数や不連続な関数が含まれている

これらの要因により，以下の次のような状況が発生し，困難になる．

- 探索空間が膨大になる
- 微分や変数を少しずつ変化させて解を探す方法が使えないので，解の候補を探すのが大変になる
- 大域的最適解と比べて良くない局所解がたくさんある

---
### 第3章のまとめ
- 変数間の関係性を，変数の定数倍とそれらの加減で記述したモデルを線形モデルという．
- 変数の間の関係性が冪乗で表されるとき，それを冪乗則という．
- モデルの式から予測される値と実際の値の差を二乗して平均をとったものを平均二乗誤差といい，これを最小化することでパラメータの値を決めることを最小二乗法という．
- 変数の値をコントロールして別のある変数の値を最小化・最大化する問題を，最適化問題という．

---
---
## 第4章 少数の微分方程式によるモデル
少数の微分方程式によって構成される数理モデルがどのように構築され分析されるのかについて解説する．

### 4.1 解ける微分方程式モデル
#### 微分方程式で時間変化を表す
対象の時間変化をボトムアップ的にモデル化したい→微分方程式によるモデリング．  
微分方程式によるモデリングは，家庭から演繹的にモデルを構成することが多い．

力学系(dynamical system): 変数の時間変化を直接記述したモデル

#### マルサスによる個体数モデル
個体数がどのように増えるかをモデル化した．
個体数を$N$，時間を表す変数を$t$とした時，個体が増える速さは$\frac{dN}{dt}$と，$N$の微分で表すことができる．
マルサスは『人口論』において，「この速度はその時点の個体数に比例する」と考えて，
$$
    \frac{dN}{dt} = rN
$$
を，個体数の増加モデルを作成した．$r$は個体が増殖する速さを表すパラメータである．

#### マルサスモデルの解の振る舞い
マルサスモデルを解くと，その解は
$$
    N(t) = N_{0} e^{rt}
$$
と求めることができる．$N_{0}$は最初の時点における個体数．
グラフを書くと分かるが，個体数は時間と共に爆発することがわかる．

#### 個体数は，爆発的に増え続けたりしない
実際には個体数は増え続けない→モデル化が間違っている．
資源は有限なので，個体数が増えすぎた場合は，個体数が減る方向に動くはず．
この効果を含めたのが**ロジスティック回帰(logistic regression)**である．
個体数増加の速度が．個体数によって変動することを加えた，以下のようなモデルである．
$$
    \frac{dN}{dt} = r(1 - \frac{N}{K})N
$$
ここで，$K$は環境収容力というパラメータ．この微分方程式を解くと，
$$
    N(t) = \frac{N_{0}Ke^{rt}}{K - N_{0} + N_{0}e^{rt}}
$$
と求まり，これをロジスティック関数(logistic function)という．

このようにして，生物の個体数変動をボトムアップ的に仮定した方程式によってモデル化することが出来た．
実際に，いくつかの生物種ではこのモデルに合致することが知られている．

#### 簡単な連立微分方程式
一般的に変数が増えると，微分方程式を解くのは非常に難しくなる．
しかし，連立微分方程式なら，変数が増えても解ける場合がある．


#### 2本の連立微分方程式
二つの変数$x_{1}, x_{2}$を用いて以下の連立微分方程式を立てると，
$$
    \frac{dx_{1}}{dt} = -x_{1} + 4x_{2} \\
    \frac{dx_{2}}{dt} = -3x_{1} + 6x_{2}
$$
これは$x_{1}, x_{2}$の時間変化は，現在の$x_{1}$の値(ブレーキのような役割)と$x_{2}$の値(アクセルのような役割)
で決まる，という状況を表している．
この連立微分方程式は，変数消去の方法で解くことができ，
$$
    x_{1} = 4C_{1}e^{2t} + C_{2}e^{3t} \\
    x_{2} = 3C_{1}e^{2t} + C_{2}e^{3t}
$$
となり，各変数の振る舞いを時間の関数として表示できる．

#### 連立1階線形微分方程式
変数を$n$個に増やした場合，ベクトル$\boldsymbol{x}$と行列$A$を用いて，
$$
    \vec{x} = \left(
        \begin{array}{c}
        x_1 \\ 
        x_2 \\
        \vdots \\
        a_n 
        \end{array}
        \right)
$$
$$
    A = \left(
        \begin{array}{cccc}
        a_{11} & a_{12} & \ldots & a_{1n} \\
        a_{21} & a_{22} & \ldots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \ldots & a_{mn}
        \end{array}
        \right)
$$

$$
    \frac{d\vec{x}}{dt} = A\vec{x}
$$

この方程式は解くことが出来て，その解は指数を$\lambda t$とする指数関数を含む関数の和で表現される．
この$\lambda$を行列$A$の固有値(eigenvalue)という．

#### 「固有値」の値によって解の性質が決まる
行列$A$の大きさが$n \times n$の時，固有値$\lambda$は$n$個存在する．
この中でも$lambda$の実数部分の値が正の場合は，時間に応じで$\lambda t$は正の値で増大して行くため，最終的に発散する．
一方で負の場合は$0$に収束する．
このように，解が時間経過と共に一定の値の範囲に収束するのか発散するのかを論じる手法は本書でも多く出てくる．
もし$\lambda$の実数部分が$0$の場合は，注意が必要．

---
### 4.2 非線形微分方程式モデル
#### 非線形な項を含む微分方程式
変数やその微分の間の掛け算を式の中にある場合，解析的に解くことはできない．
しかし，微分方程式を解かなくても振る舞いをある程度調べることは可能である．

#### ロトカ・ヴォルテラ方程式
捕食者・被食者の個体数変動をモデル化．
ロトカ・ヴォルテラ方程式(Lotka-Volterra equation)
$$
    \frac{dx}{dt} = x(r - ay) \\
    \frac{dy}{dt} = y(-s + bx)
$$
$x$が被食者の数，$y$が捕食者の数，$a, b, r, s$は正のパラメータ．

#### 「定常」な状態について考える
定常解：時間変化しない状態．
先ほどの方程式に，$0$を代入して，微分を含まない連立方程式を解くと，
$$
    x = 0, y = 0
$$
または
$$
    x = s/b \\
    y = r/a
$$
となり，両者ともに存在しない時か，被食者が自然に増える速さと捕食者に捕食される速さが一致し，かつ捕食者が自然に減っていく速さと捕食によって増えていく速さが一致するというバランスが取れた状態を表現．


#### 定常解の「安定性」
安定性(stability): 微小変化した時に，状態が元の定常解に戻ろうとする時．
逆に，どんどん誤差が拡大していく場合は不安定．

#### 安定性を実際に評価する
ロトカ・ヴォルテラモデルにおいて，被食者と捕食者がいない定常解
$$
    x_{0} = 0, y_{0} = 0
$$
について，微少変化を$\epsilon_{x}, \epsilon_{y}$を与えた時にどうなるかを検討する．
システムの状態$(x, y)$は
$$
    x = x_{0} + \epsilon_{x}, y = y_{0} + \epsilon_{y}
$$
であり，ロトカ・ヴォルテラ方程式に代入して整理すると，
$$
    \frac{d\epsilon_{x}}{dt} = r\epsilon_{x} \\ 
    \frac{d\epsilon_{y}}{dt} = -s\epsilon_{y}
$$
となる．この微分方程式を解くと，
$$
    \epsilon_{x} = C_{1}e^{rt} \\ 
    \epsilon_{y} = C_{2}e^{-st}
$$
となり，$r, s$の正のパラメータであるため，$\epsilon_{x}$は発散する．
よって
$x_{0} = 0, y_{0} = 0$
は安定ではないことがわかる．
被食者は捕食者がいないと増え続けることができ，捕食者は被食者がいないと増えることができない．

#### 微分方程式の線形化による解析
定常状態からの微小誤差についての方程式を作ると，二次以上の非線形な項を無視することができる．
つまり，非線形の項を全て消去することができ，結果的に非線形の微分方程式が線形の方程式になる．
この手続きを**線形化**といい，線形化によって状態の安定性を評価することを
**線形安定性解析**(linear stability analysis)という．

定常解と安定性の議論は，かなり汎用性の高い手法．

#### 数値シミュレーション
直接解けない場合でも数値計算を行うことで，実際にその方程式に従う状態の時間変化を見ることができる．
$$
    \frac{dx}{dt} = x(r - ay) \\ 
    \frac{dy}{dt} = y(-s + bx)
$$
ある時間$t_{0}$における個体数を$x_{0}, y_{0}$とした時，
$x, y$のそれぞれの変化速度$v_x, v_y$は
$$
    v_x = x_{0}(r - ay_{0}) \\ 
    v_y = y_{0}(-s + bx_{0})
$$
と表すことができる．
そこから微小時間経$\Delta{t}$経った時の，それぞれの個体数$\hat{x}, \hat{y}$は
$$
    \hat{x} = x_{0} + v_x\Delta{t} \\ 
    \hat{y} = y_{0} + v_y\Delta{t}
$$
より，
$$
    \hat{x} = x_{0} + x_{0}(r - ay_{0})\Delta{t} \\ 
    \hat{y} = y_{0} + y_{0}(-s + bx_{0})\Delta{t}
$$

と計算することが可能である．

このように$\Delta{t}$をとって値を更新していく方法をオイラー法による数値積分という．
またより誤差を小さくしたルンゲクッタ法(Runge-Kutta method)が実用的に用いられる．

**数理モデリングでは，実際にそのモデルがどのように振る舞うかを調べるのは必須である．**


---
### 4.3 解けるモデル・解けないモデル
#### 解ける微分方程式は少ない
解ける微分方程式の目安  
- 微分方程式が線形である
- 独立変数以外の変数が一つだけの微分方程式なら，多少非線形でも解ける

#### 線形の微分方程式
定数項や高階の微分が含まれていても解ける．

#### 1変数の非線形の微分方程式
- 変数分離形
- 同次形
- ベルヌーイの微分方程式
- ラグランジュの微分方程式

#### 偏微分方程式
別の変数による偏微分を含む微分方程式のこと．
拡散方程式(diffusion equation):
$$
    \frac{\partial C(x, y)}{\partial t} = D \frac{\partial ^2 C(x, t)}{\partial x^2}
$$
$C$は着目している熱や物質の量，$D$は拡散係数を表すパラメータ，$t$は時間，$x$は位置を表す独立変数．

#### 分析ソフトウェアの利用
常微分方程式を解析的に解くソフトウェアも数値シミュレーションをソフトウェアもある．

---

### 4.4 制御理論
#### 入力変数に対してシステムがどう応答するか
- 制御工学(control engineering): 入力及び出力を持つシステムにおいて，その出力を自由に制御する方法全般にか関わる学問．
- 制御理論(control theory): 数理モデルを対象とした，主に数学を用いた制御に関する理論．
    - モデルの表現方法，解析手法(安定性など)，制御形設計方法
- 制御応用: 実モデルに適応していく．


#### 微分方程式を解くための便利な道具
ラプラス変換(Laplace transform): 積分計算することで別の関数に変換する．「関数の微分」を変換することが重要．

#### 微分方程式をラプラス変換で解く
1. 元の微分方程式をラプラス変換する
2. ラプラス変換後の世界で解く
3. 元の世界に戻す

このようにすることで，簡単に微分方程式を解くことが出来る．

#### 微分方程式に制御項を入れてみる
微分方程式に制御できる項$u(t)$をいれて，ラプラス変換を行い，変換すると
$$
X(s) = G(s)U(s)
$$
制御できる変数$u(t)$のラプラス変換$U(s)$は，一つの関数を決めると一つの具体的な関数が入る．
$G(s)$を伝達函数という．ここに，システムの安定性情報が含まれていて，これを分析することで色々分かる

#### 目的の値を達成するためのフィードバック系を考える
PID制御(Proportional-Integral-Differential Controller)

目標値を目指して制御している時に，ある時刻における目標値からのずれを0に近づけるために，3つの要素に従って入力の値を決める．

- 比例要素(P): 現在のずれに比例した値を入力に加える
- 積分要素(I): 今までのずれを時間的に積分し，それに比例した値を入力に加える
- 微分要素(D): ずれの変化速度に比例した値を入力に加える

比較的実装しやすく，パラメータの値を適切に設定すると非常によく機能する．

#### 古典制御理論と現代制御理論，その後
ラプラス変換や伝達関数は古典制御理論という．
それ以降は，観測できない変数(状態変数)や多変数のダイナミクスを考慮にいれることが可能な現代制御理論につながっている．

---
### 第4章のまとめ
- 変数の変化の速度を表す微分を含む式を微分方程式といい，変数の動的な振る舞いを理解するのに使用される
- 線形の微分方程式では，変数の時間変化を時間の関数で表示することができるが，非線形の方程式の場合解けないことが多い
- 解けない微分方程式でも，定常状態が維持されやすいかどうかを安定性の議論によって調べることが出来る
- 線形の微分方程式の場合，変数の値をコントロールする制御理論が整備されている

---
---
## 第5章 確率モデル
対象のダイナミクスが確率的な振る舞いが本質的である場合には，確率モデルが力を発揮する．

### 5.1 確率過程
#### 確率的な状況を考える
微分方程式は同じ条件からスタートした場合，毎回同じなる(決定論的なダイナミクス)．
現実のデータはばらつきがあり，それが無視できない場合は，確率的な要素をもつモデルの方が対象の実態をよく捉えることが出来る．

#### 確率分布は確率の情報をひとまとめにしたもの
確率分布(probability distribution): 確率変数がとる値ごとに，それが実現する確率をひとまとめにした関数．
確率変数(random variable): 実際に出る値．

#### 連続変数の確率分布
単独の値が起こる確率は0だが，幅を持たせることで0でない値を定義できる．

#### 連続変数の確率分布: 確率密度関数
確率密度関数(probability density function): 簡単にいうと確率分布のこと．
確率変数$X$が，$a$から$b$に入る確率は確率密度関数$p(x)$を用いて，以下の式で表すことができる．
$$
P(a \leq X \leq b) = \int_{a}^{b}p(x)dx
$$

ヒストグラムをどんどん小さくしていくと確率密度関数が出来上がるイメージ．

#### 何度もサイコロを振ってみる
確率過程(stochastic process): 時間変化していく確率変数のこと


---
### 5.2 マルコフ過程
#### マルコフ過程は，過去の状態を振り返らない
マルコフ過程(Markov process): システムの確率変数(状態)を変化させる時に，現在の状態の情報だけで次の状態を決めること

#### ランチの決め方
今日食べたランチによって，次の日に食べるランチを決める．

マルコフ過程において，離散的に時間が進む場合を，特にマルコフ連鎖(Markov chain)という．

#### どれくらいの割合でラーメンを食べることになるか
日々のランチが，確率的にきまっていくが，この状態が変化していくことを状態遷移(state transition)といい，
それぞれの状態遷移が起こる確率を遷移確率(transition probability)という．

#### 状態確率の方程式を立てる
状態確率: ある時間でのそれぞれの確率変数が起こる確率

$$
(次の状態確率) = (状態遷移行列) \times (前の状態確率)
$$

前の状態確率に行列を掛け算すると次の状態確率が出てくる．
ある時刻における状態は，初期条件の状態確立に遷移行列を$t$回掛け算することで求めることが出来る．

#### 時間が十分経過した後の話
ある一定の条件を満たせば，状態確率は時間変化しない一定の値に収束する．この状態を定常状態という．
定常状態を考えると，簡単に確率や期待値を計算することができ，システムの分析に役立つ．


---
### 5.3 待ち行列理論
#### 窓口の行列を確率的に表現する
あるコンビニにATMを設置したいが，一台で足りるか？

これを数理モデルを使って評価する

#### 平均的な振る舞いに基づいた推測
最も簡単なアプローチは平均的にどれくらいの利用者がやってきて，そして平均的に一回の利用でどれくらいの時間ATMを占有するかを調べること．
新しい利用者が到着するまでの時間が一人当たりの利用時間より長ければ大丈夫そう．

しかし，この推論には問題がある．それは利用者がやってくるタイミングや利用時間が一定ではなく，人によってばらついていることを考慮できていない．

#### ランダムに利用者が到着することを表現するポワソン過程
ATMに利用者が到着する頻度を1時間あたり$\lambda$とする．これはまだ，実際にどのタイミングで利用者がやってくるかはわからない．

この時に，最もランダムな到着の仕方として，次のように考える．

微小時間$\Delta t$で1時間を分割すると考えて，そのそれぞれの時点で確率$\lambda \Delta t$でランダムに利用客が到着すると考える．

このようにして得られた事象を時間に従ってカウントしていく確率過程をポワソン過程(poisson process)という．

#### 増えたり減ったりを表現する出生死滅過程
これまでに利用者が到着する様子をモデル化することができた．次に，利用者が利用していなくなる要素を含める．

利用者がいる時に，サービスが完了する速さを$\mu$とする．
ポワソン過程では，微小時間$\Delta t$の間に確率$\lambda \Delta t$でATMの列に新しい利用者がやってくるが，
さらに，確率$\mu \Delta t$で列のATMの先頭にいる利用者がいなくなる確率過程を考える．
これを**出生死滅過程**(birth-death process)という．

#### 行列の長さを確率的に捉える
出生死滅過程によって，行列の長さが確率的に変化する様子を分析する．
人数がランダムに増減するので，人数を直接扱うのは難しい．
そこで，時刻$t$に行列が$n$人である確率$P_{n}(t)$を導入して，計算する．
行列に並んでいる人数$n$人のパターンは無数にあるが，どれも同じ構造なので，一般に$n$人の時だけを調べる．

$$
P_{n}(t+\Delta t) = \lambda P_{n-1}(t)\Delta t + (1 - \lambda \Delta t + \mu \Delta t)P_{n}(t) + \mu P_{n+1}(t)\Delta t
$$
左辺第1項は新しく到着した場合，第2項は増減なし，第3項は完了して減った場合．

$\Delta t$が非常に小さい極限をとると，微分方程式として，表現出来る．
$n$が無数にある線形の微分方程式であり，時間の関数として解けない．
この方程式が時間の経過によって定常解に近づくのでその定常的な振る舞いを考える．

#### 定常状態の分析
時間に関係なく$P_{n}(t)$は変化しなくなる(微分=0)．三項漸化式や無限等比級数を解いていくと，
$$
P_{n} = (1-\rho)\rho^n
ただし，\rho = \lamda / \mu
$$

#### 確率分布がわかったら，期待値がもとまる
確率分布がわかれば，たいていの期待値(e.g. ATMの稼働率，平均の行列の長さなど)を求めることが可能．

#### 確率モデルの強みと限界
確率過程を数理構造にもつモデルは，
**システムにおけるばらつきの効果が無視できない場合に，直感的には予測が難しい性質を捉えることが出来る．**

気をつけるべきこととして，このモデルで仮定した出生死滅過程が，現実のシステムと必ずしも精度良く一致しているとは限らないこと．
例えば，利用者の到着などを決める確率分布は常に一定ではなく，時間によって変化するかもしれないし，ATMを2台設置した場合を考えたいかもしれない．

このような家庭のうち，特に数理的な解析の難易度を大きく変えるのは確率過程の選択である．
過去の状態を含めないマルコフ過程は，数理的に解析しやすいが，それ以外の確率過程は非常に難しくなる．

#### 数値的な解析は比較的容易
モンテカルロ・シミュレーション(Monte Carlo simulation):確率モデルの振る舞いをコンピュータでシミュレートする際に，プログラムに乱数を発生させて，それに従って実際にモデルを動かすこと．

#### マルコフ近似
マルコフ性を満たさなくても，マルコフ性を満たすとして計算し，その後整合性をチェックする方法がある．

---
### 第5章のまとめ
- 変数の振る舞いを確率的に記述したものを確率モデルという
- 変数の確率的な振る舞いが，一つ前の時点の状態のみに依存するものをマルコフ過程といい，理論解析のしやすさからよく使用される
- システムが十分時間が経った後に達する定常状態は，理論的な解析が容易に出来る
- システムの状態の確率分布が求まれば，そこからさまざまな量の期待値を計算することが出来る


---
---
## 第6章 統計モデル
データのばらつきに対処しつつ推論を行うための方法論が統計科学である．

### 6.1 正規分布
#### 双六の結果
サイコロを一回振った時の目の出方は一様分布(uniform distribution)，
サイコロを何回か連続で振った場合は真ん中が高くなるようになる．

#### 正規分布は確率分布の親玉
このようにサイコロを何回も振ってその和をとっていくと，確率分布は正規分布(normal distribution)に近づく．
これを中心極限定理(central limit theoram)という．
つまり，一般的な確率分布から独立に生成された確率変数を足していけば，その和の確率分布は正規分布に近づく

#### 正規分布の定義
二つのパラメータ: 平均(mean)と標準偏差(standard deviation)で表現され，
$N(\mu, \sigma ^2)$と書く．

#### 異なる正規分布同士の足し算も正規分布
異なる正規分布から生成される確率変数の足し算は，それぞれの平均と分散を足した確率分布の従う．

#### 正規分布の二乗の足し算は?
世紀分布から生成される確率変数を二乗したものの和は，$\chi ^2$分布に従う．
$\chi ^2$分布に従う確率変数の比をとると，$F$分布に従う．

---
### 6.2 統計的検定
#### ばらつきについて考える
得られるデータは常にばらつき・誤差を含んでいる．
このばらつきが発生する確率に焦点をあてることで，データに見られる特徴が偶然起きたのか，それとも何らかの意味があるのかを評価出来る．

#### 偶然の確率
コインを10回投げて，表が9回出た．
これは偶然なのか，コインに仕掛けがあって表がでやすくなっているのか？

- 帰無仮説(null hypothesis): 検定を行うために立てる仮説
- 対立仮説(alternative hypothesis): 帰無仮説に相対する仮説
- p値: 帰無仮説を仮定した時に，今回得られたデータが発生する確率
- 有意水準(signigicance level): 有意か否かを判定する基準．

#### 平均に関する検定
今得られた$n$個のデータが独立に正規分布から生成されたとする(数理モデルを仮定している)．
この時，このデータが従っている真の確率分布の平均値は$\mu$と異なっているか？を検定したい．
なお，真の確率分布の分散$\sigma ^2$が分かっているとする．

この時，標本平均を$\bar{X}$として，次の値を計算する．
$$
Z = \dfarc{\bar{X} - \mu}{\sigma / \sqrt{n}}
$$
この$Z$は，帰無仮説が正しければ，標準正規分布に従うとされ，そこからこのずれが発生する確率を評価することが出来る．

検定に使用する量のことを検定統計量(test statistics)といい，統計検定ではこの検定統計量が特定の確率分布に従うことを利用して，
そのような値が実現する確率を求める．
またデータの真の分布として正規分布などを仮定できない場合は，ノンパラメトリック検定が利用される．

#### 統計検定は万能ではない
統計検定は「帰無仮説の上でデータの事象が偶然起きるにしては確率が低すぎる」と言っているだけ．

帰無仮説が棄却されなかった場合は，帰無仮説が正しいと示したわけではなく，帰無仮説では説明できない，と言っているだけ．

有意水準が0.05と言っても，20回に1回は偶然起こりうることに注意が必要(検定を何度も行うと有意になることがある: 多重生の問題)

#### 第一種過誤と第二種過誤
- 第一種過誤(type 1 error): 本当は帰無仮説が正しいのに，誤って帰無仮説を棄却してしまうこと．有意水準
- 第二種過誤(type 2 error): 本当は対立仮説が正しいのに，誤って帰無仮説を採択すること．検出力．

統計検定ではこの二つを同時に小さくすることはできない．

#### 統計検定の実施
Rm Python, Excel, HAD, SPSS, JPMなど


---
### 6.3 回帰分析
#### 線形モデルの妥当性
統計モデルの視点から見る

#### 同じ線形モデルでも・・・
別々に得られたデータ(データ1, データ2)において，最小二乗法を用いて，線形モデルを作成した．
この時，全く同じ関係式が得られた．
しかしながら，データ1の方が散らばりが少なく，よく説明できてる．

この時，線形モデルの当てはまりの良さを表す指標として，決定係数(coefficient of determination)がある．
これは，モデルと測定値の誤差と全体のばらつきの比率を求め，そこから，線形モデルがどれだけ良く当てはまっているかを見ることが出来る．

#### ピアソン相関係数による評価
ピアソン相関係数(Pearson's correlation coefficient): 二つの変数の連動性を定量化した指標．
あくまでも線形の関係でデータがどれだけ説明できるかを示している．

#### 当てはまりの良さだけで結論していい?
線形回帰を決定係数や相関係数などの当てはまりの良さだけで評価してはいけない，

相関係数$r$に基づいて，行う方法と回帰係数$\beta$に基づいて行う方法がある，

線形回帰した式とそれぞれのデータ点をの誤差が同一の世紀分布からランダムに発生していると仮定する．
その上で相関係数が$r=0$，もしくは，回帰係数$\beta = 0$を帰無仮説に設定する．
そして検定を行う．

#### 変数の数が増えた場合は重回帰分析
説明変数が複数ある場合は重回帰分析になる

#### 線形でないモデル
線形モデルは

- 変数の間の関係が線形(直線)の関係式で表現できる
- 誤差が常に同じ正規分布に従う
という強い仮定をおいている，

このような仮定が成立しない時は，一般化線形モデル(generalized linear model)を利用すると良い．

---
### 第6章のまとめ
- 正規分布は確率分布の親玉的な存在で，様々なところで利用されている
- 仮定した確率モデルを使って，データの背後にある真の分布に関する推論を行う手続きのことを，統計的検定という
- 線形モデルの妥当性を統計的検定によって評価することができる
- 一般化線形モデルを使えば，モデルの非線形性やばらつきの分布を自由に選択できる

---
## 第二部のまとめ
方程式モデル，微分方程式モデル，確率モデル，統計モデルについての基礎事項を学んだ．
最適化，微分方程式，安定性，確率過程，定常状態の解析，統計的検定はより高度なモデルでも使われる．

---
---
---


# 第三部 高度な数理モデル
時系列モデル，機械学習，強化学習，多体系モデルなどの実戦で多用されるモデルを紹介する．
それ以外にも，次元削減やネットワーク科学，非線形時系列解析などの複雑なシステムを分析する際に重要な解析法についても紹介する．

---
## 第7章 時系列モデル
時系列データは，他の種類のデータには特有の性質があり，分析の際に注意が必要である．

### 7.1 時系列データを構成する構造
#### さまざまな時系列
時系列といってもさまざまある．
ガス使用料や株価指数，カオスなど．
規則的な変動を周期変動という．

#### いろいろなアプローチ
時系列データのバリエーションは豊富．
それぞれに対して，アプローチの方法があり，それを紹介する．

#### トレンド＋周期成分＋ノイズ
- トレンド(trend): 比較的長い感覚で見た時の増加/減少傾向のこと
- 周期成分: 周期的な変動
- ノイズ: 説明できない誤差

#### 周波数成分
時系列データを波の集まりとして捉える．
波は，三角関数もしくは複素数の指数関数で表現することが可能
周波数は波の細かさを表現しており，特定の原因から発せられる振る舞いやノイズは特有の周波数を持っていることがある．
データにどうのような周波数が含まれているかを調べるのは有力な方針である．
波を表す基本の式
$$
Asin(\omega t + \varphi) = Ae^{i(\omega t + \varphi)}
$$
ただし，$A, \omega, \varphi$はそれぞれ，振幅，角周波数，位相である．

#### 特定の非線形構造がある場合
非線形時系列解析: 非線形なシステムの一部の特徴を利用して高度な分析を行うこと

#### 時系列が定常か
多くの時系列解析では，時系列データが確率過程から生じた現実値として解析する．
この時，データの背後にある確率過程自体が時間的に変化している場合(明確なトレンドや平均が時間で変化する)はうまく分析できないので，
それを取り除く必要がある．

#### 時間を説明変数にして普通に統計検定を行ってはいけない
多くの統計検定は，ノイズが正規分布と仮定したり，隣接した点が独立にノイズが決まると仮定しているが，
時系列データは隣接する点に関係があることがある．


---
### 7.2 観測変数を使ったモデル
#### 予測に使えるモデルたち
時系列データ分析の問題設定の一つは，「過去のデータに基づいて未来を予測すること」．
これからは，各モデルの概略を紹介する

#### ARモデル
データ点の間に何らかの時間的な関係をモデル化する．

$$
x_t = c + \phi x_{t-1} + \epsilon_t
$$
ただし，$c, \phi$は定数パラメータ．
$\epsilon$はノイズ項であり，平均が0，分散が$\sigma^2$の確率分布から毎回独立に決定されるとする．
これをホワイトノイズという．
つまり，
$x_t$は，平均が$c + \phi　x_{t-1}$，分散が $\sigma^2$のノイズ分布から生成されると，解釈することが出来る．
この関係性を仮定したモデルを自己回帰モデル(autoregressive model)，ARモデルという．

さらに，過去p時間まで遡って変数に加えたモデルをAR$(p)$と表現し，
$$
x_t = c + \phi_1x_{t-1} + \phi_2x_{t-2} + ... +  \phi_px{t-p} + \epsilon_t
$$
となる．

また，この変数を多変数のベクトルにも拡張できて，それをベクトル自己回帰(vector autoregressive: VAR)モデルという．

#### ARMAモデル
ARモデルとは異なり，ある時点でのノイズが$q$時点後まで影響を与えると仮定したモデルを，
ARMA(autoregressive moving average model)という．
$$
x_t = c + \Sigma_{i=1}^{p} \phi_ix_{t-i} + \epsilon_t + \Sigma_{i=1}^{q}\Phi_{i}\epsilon_{t-i}
$$
新たに加えた過去のノイズの項$\Sigma_{i=1}^{q}\Phi_{i}\epsilon_{t-i}$を，移動平均(moving average)という．
これで，普通の線形回帰では表現できないノイズの間の関係性を表現できる．

#### ARIMAモデル
ARモデルやARMAモデルでは，定常生が満たされていない場合は使えない．
しかし，各時刻で前後の値の差をとって(差分)，各時刻での変動分の時系列を作ると，近似的に定常と見なせることがある．
この手続きによってトレンドを除くことができる．
このように，差分をとってからARMAモデルを適用することをARIMA(autoregressive integrative moving average)モデルという．

#### SARIMAモデル
周期変動を除いて作成した時系列に対してARIMAモデルを適用する方法を，
SARIMA(seasonal autoregressive integrative moving average)モデルという．
古典的な時系列モデルだが，適切に使用すればパフォーマンスを発揮する

まとめると

- AR: 自己回帰の関係性を記述
- ARMA: 移動平均の効果を追加
- ARIMA: トレンドの効果を追加
- SARIMA: 周期(季節)変動の効果を追加


---
### 7.3 状態空間モデル
#### 状態変数を含むモデル
時系列データを分析する際に非常に強力な手法の人一つが，
状態空間モデル(state space model)．
これは非常に広い概念で，個別のモデルではない．

- 状態変数(state variable): 観測されない潜在変数のこと．
- 観測変数: 実際にデータを観測できる変数のこと．

この状態変数を組み込むことで，定常性を満たさない時系列に対しても，モデルを適用することが出来る．
状態空間モデルは，非常に汎用性の高い手法である．

#### 状態空間モデルの一般的な表現
システム方程式(system equation): 状態変数(ベクトルでも可)の時間変化を記述する方程式．
観測方程式(observation equation): ある時刻に状態変数から観測変数(ベクトルでも可)が生成される関数のこと．

状態空間モデルはこのシステム方程式と観測方程式を合わせたもの．

#### 離散時間・線形・ガウスモデル
線形ガウス型常態空間モデル(動的線形モデル dynamic linear model: DLM): 
最もスタンダードな常態空間モデル．時間が離散的に与えられている場合．

$$
\left\{
    \begin{array}{1}
        x_t = G_tx_{t-1} + w_t
        y_t = F_tx_t + v_t
    \end{array}
\right.
$$
ただし，
$x_t$が潜在変数，$y_t$が観測変数，
$G_t, F_t$は時間に依存しても良い係数行列で，
$G_t$を状態遷移行列(state-transition matrix)，
$F_t$を観測行列(observation matrix)という．
$w_t, v_t$は正規分布に従うノイズ項．

動的線形モデルを行うには，RのdlmやPythonのstatsmodelsやPyDLMがある

#### その他の場合の状態空間モデル
線形性やノイズに対する正規性，離散時間を仮定したが，そういう仮定がいらないモデルもある．

連続時間の状態空間モデルは，制御理論で深く研究されている．
特に「どのようにしてシステムの状態を所望の状態にすることができるか」に焦点を当てた方法論は，
一般のデータ分析においても，非常に有用である．


---
### 7.4 その他の時系列分析法いろいろ
#### 自己相関で時間構造を特徴づける
自己相関(autocorrelation): 着目する変数が$\tau$時点離れた点どうしてどれくらい似ているかを表した指標．
周期的な変動を見ることが出来る．

#### 異常拡散による特徴づけ
時系列において，$\tau$だけ時間が離れた時に値の変化量を$\Delta x$として，
その$\Delta x$の標準偏差$\sigma(\Delta x)$を考える．
この量は，時間が離れた時に，どれくらい結果がばらついて予測できなくなるかを示している．
各時点でのノイズが完全にランダムに独立である場合は，この量は$\tau ^0.5$に比例する．
変数の値がこのように広がっていくことを拡散(diffusion)という．

一方で，この指数が0.5から離れることがあり，これを異常拡散(anomalous diffusion)という．
この異常拡散を捉える方法として，ハースト指数(Hurst exponent)$H$があり，
$$
\sigma(\Delta x) \propto \tau^H
$$
である．この$H$によって，ノイズが過去のノイズと関係性しているか(記憶性)を特徴づけることが出来る．

#### フーリエ変換による周波数解析
フーリエ変換(Fourier transform): データにどのような波がどれくらい含まれているのかを分析する．
元の関数を時間の関数から周波数の関数に変換し，それぞれの周波数がどれくらい使用されているかを見ることで，データの特徴を把握する．
パワースペクトルという量を見ることで，特徴的な音の高さに対応する周波数の成分が多く含まれていることがわかる．

それ以外にも周波数の成分が時間的に変化するデータの場合，スペクトログラム分析やウェーブレット変換などの手法も用いられる．

#### カオス・非線形時系列解析
様々な時系列データでカオスが見られる．
ここではカオスの定義については触れないが，簡単にいえば，非周期ダイナミクスのこと．

遅れ座標(delay coordinate): 時系列データの前後いくつかの点をひとまとめにして，n次元の空間の中の一点と捉える方法．
カオス時系列に対して特定の構造が見える場合がある．

シンプレックス・プロジェクション(simplex projection): その構造を利用して，時系列データをモデルなしで予測・説明する強力な方法．

リアプノフ指数(Lyapunov exponent): 時系列がカオス的であるかどうかを評価する指標．
まず値が近い二つの異なる状態を考え，普通のシステムなら似たような状態変化を辿るが，カオスならその差が拡大していく．
この離れていく度合いを定量化するのがリアプノフ指数で，初期値が違いのに時間が経つと大きく異なる性質を初期値鋭敏性という．

#### 2つ以上の時系列から因果関係を調べる
時系列データにおいて，ある変数が別の変数に影響を与えているかを知りたい．

- 移動エントロピー(transfer entropy): 情報量の観点から，XがYに影響を与えているなら，Xの情報を使うことでYの予測精度を上げられると考える．
- Granger因果(Granger causality): VARモデルなどを利用して，予測の精度を評価する・
- CCM(convergent cross mapping): 遅れ座標によるシンプレックス・プロジェクションを2つの時系列間で行う

一般的に，データから因果関係を推定するのは非常に難しく，データの性質に応じて適切に手法を選ばないと誤った結論を簡単に導いてしまう．
「この手法さえ使っていれば間違いない」という方法は存在しないので，適用条件については深く検討する必要がある．

---
### 第7章のまとめ
- 時系列データには，トレンドや周期変動などの時間的な構造が含まれていて，通常の統計分析ではうまく分析できないものもある
- 観測変数の間の関係性を直接モデル化したものとして，ARモデル，ARMAモデル，ARIMAモデル，SARIMAモデルなどがある
- 状態空間モデルでは，状態変数を用いることで，より自由度の高いモデリングが可能となる
- 時系列データの分析法には，着目する性質に応じて，他にも周波数解析，非線形解析，因果性解析などさまざまなものがある

---
---
## 第8章 機械学習モデル
応用志向型モデリングの本丸．
機械学習モデルの基本的な思想は，目的を達成するための方法をデータから機械に学ばせるという考え方．

### 8.1 機械学習で扱われるモデル・問題の特徴
#### 機械学習の基本的な考え方
数理モデルの中でも，実応用におけるパフォーマンスを重視するのが機械学習．
複雑な現実問題を精度よく記述することが必要なため，多くのパラメータを含む複雑なモデルが必要な場合が多い．

#### 複雑な問題，複雑なモデル
高次元なデータから低次元で非線形な特徴を捉えるためには複雑なモデルが必要．

#### モデルの自由度とオーバーフィッテイング
過学習(overfitting): パラメータ推定した時に使ったデータ(訓練データ)にはよく当てはまるものの，新しく得られたデータ(テストデータ)に対しては当てはまらないこと．
モデルをデータのばらつき・誤差に合わせすぎてしまうのが原因．

汎化(generalization): モデルが過学習せずに，未知のデータにもよく当てはまること．

#### 機械学習モデルを使った分析の実施
Pythonのscikit-learnやTensor-Flow, Kerasなど．
大きな計算資源が必要になることもある．

---
### 8.2 分類・回帰問題
#### 分類と回帰
- 分類(classification): ラベルを予測する
- 回帰(regression): 値を予測する

#### 決定木
決定木(decision tree): 学習データから，データを最もうまく分類できる条件の組を学習する

利点: わかりやすい，結果の解釈が容易

欠点: 過学習しやすい

#### ランダムフォレスト
ランダムフォレスト(random forest): 決定木をたくさん生成して，それらの多数決で決定する方法(アンサンブル学習; ensmble learning)．
元のデータセットからランダムにサンプルを取り出し(ブートストラップ)，それについて決定木を作り，多数決をとる．

#### サポートベクターマシン
サポートベクターマシン(support vector machine: SVM): データをクラスごとにプロットし，それぞれのクラスを直線(平面，超平面)で分類する方法．このようにデータが直線(平面・超平面)で分離できることを，線形分離可能である，という．データに非線形な変換を施すことで，線形分離可能な問題に帰着させて適用することも可能．

#### ニューラルネットワーク
ニューラルネットワーク(neural network): 単純な計算を行う要素(ノード)をネットワーク状に組み合わせて，活性化関数(activation function)を通してノード間のやりとりを行い，最終的に値・ラベルを予測する方法．
単純な非線形関数を組み合わせて何度も適用することで極めて高い表現力を得ることが出来る．

- 入力層: データを入力する層
- 出力層: データを出力する層
- 中間層: 入力層と出力層の間の層

この中間層を増やしたものを深層学習モデル(deep learning model)という．

---
### 8.3 クラスタリング
#### クラスタリングでデータを解釈
クラスタリング: データ点の散らばり具合だけを見て，近いデータたちを同じカテゴリにまとめる手続きのこと．教師なし学習．
クラスタリングアルゴリズムの選択やクラスター数の仮定などによって結果が変化するため，常に任意性があることを忘れない．

#### k-means法
k-means法: データ点と各クラスターの中心からの距離を比較して，一番近いクラスターに分類させる．

1. クラスター数kを適当に決める
2. 各データ点を適当にランダムに各クラスターに割り当てる
3. それぞれのクラスターの中心点を求める
4. 各データ点について，一番近い中心点を探し，そのクラスターに割り当てる．
5. 割り当てが変化しなくなるまで3, 4を繰り返す

#### 混合分布モデル
k-means法では，データの生成規則に関して特に数理モデルを仮定しない．

混合分布モデル(mixture model): 各クラスターのデータを生成する確率分布をそれぞれ仮定して，そのどれかからデータが生成されている，と考える方法．
特にガウス分布でデータの確率分布を表現するものを混合ガウスモデル(Gaussian mixture model)という．

混合されている確率分布を推定すれば，そのそれぞれが各クラスターに対応する．

データの生成分布 = クラスタ1の確率分布 + クラスタ2の確率分布 + ... + クラスタkの確率分布

この右辺を混合分布モデルという．

#### 階層的クラスタリング手法
階層的クラスタリング(hierarchical clustering): クラスター間の類似度をもとに，クラスターのまとまり方を調べる方法．
類似度の計算にはさまざまな方法がある．


---
### 8.4 次元削減
#### 次元削減とは
次元削減(dimensionality reduction): 理解できない高次元データを，本質的な情報を失わずに低次元のデータで表現すること．

#### 主成分分析
主成分分析(principal component analysis; PCA): データと最もばらつく直線を見つけ，その直線を新たな座標軸とする手法．
次に，その直線と直行する直線を新たな座標軸とする．
つまりデータがばらつけばばらつくほど，情報量が多いということ．
直線的なデータのしか捉えられないため，非線形な特徴を次元圧縮することはできない．

#### 独立主成分分析
独立主成分分析(independent component analysis; ICA):
主成分分析とは異なり，必ずしも直行する直線を取る必要がない．
予め任意の成分の個数を指定して，データをその数の独立した成分で表現すること．
事前にデータがいくつの成分に分けられるかがわかっている場合有用であるが，任意性が残る．

#### 非線形な次元削減法
- カーネルPCA: データに非線形変換を行い，主成分分析を行う
- 多様体学習(manifold learning): データの非線形な構造に沿って次元圧縮する．各データ点の近傍にあるデータから多様体という構造の情報を計算して，次元に圧縮に用いる方法．Isomap，LLE，t-SNEなどのアルゴリズムがある．
- 位相的データ解析(topological data analysis): データの「かたち」に着目した次元削減法

---
### 8.5 深層学習
#### 深層学習とは
深層学習(deep learning): ディープニューラルネットワーク(deep neural network)を用いた機械学習．
学習アルゴリズムの発展，大量学習データ取得の容易化，GPUやメモリなどの性能向上によって，注目を浴びている．
モデル自体の解釈が非常に困難であり，非常に応用志向型のアプローチである．

#### 畳み込みニューラルネットワーク
畳み込みニューラルネットワーク(convolutional neural network; CNN): 畳み込み層とプーリング層という中間層を持つ．
ある対象のパターンの位置が変化しても，同じように処理することが可能になる．

#### リカレントニューラルネットワーク
リカレントニューラルネットワーク(recurrent neural neteorl; RNN): 順方向だけでなく，後ろに戻る経路をネットワークに加えた方法．
過去の中間層の値を保持したり，過去の出力を中間層に戻したりするなどの，さまざまな方法が知られている．

#### オートエンコーダ
オートエンコーダ(autoencoder): 入力と出力が同じになるように学習させたニューラルネットワーク．
入力層→中間層で少ない数の変数で表現され(エンコード)，中間層→出力層で元に戻す(デコード)ことで，中間層では必要な情報を出来るだけ失わずに次元圧縮ができたことになる．

#### 敵対的生成ネットワーク
敵対的生成ネットワーク(generative adversarial network; GAN): 生成器(generator)と識別器(discriminator)を同時学習させ，生成器では画像を生成し，識別器ではそれを見抜くことを目標にして学習させる，つまりライバル関係にある2つのモデルを学習させる．
最終的に，本物と見分けがつかないデータを生成することが可能になる．

---
### 第8章のまとめ
- 機械学習では，応用時の性能を重視し，パラメータを多く含む複雑なモデルを利用する
- 学習データにモデルを合わせすぎてしまい，その他のデータに対して性能が出なくなることを，過学習という
- 機械学習モデルが解決する課題には，代表的なものとして，分類，回帰，クラスタリング，次元削減がある
- 深層学習は学習にかかるコストは大きいが，難しい問題を解決するためのパワフルな手法である

---
---
## 第9章 強化学習モデル
強化学習とは，環境からのフィードバックに応じて最適な反応を探索するためのフレームワーク．

### 9.1 行動モデルとしての強化学習
#### 強化と学習
強化学習(reindorcement learning): トライアンドエラーを何度も繰り返しながら適切な行動を学習していく時間変化を数理モデルで表現したもの．
モデル化される意思決定主体をエージェント(agent)という．
行動の結果，正負どちらかの報酬が得られるが，その行動の結果を決める場所を環境(environment)という．

#### ギャンブル課題
カードを引いて書かれた数字の分だけお金がもらえるギャンブルを考えて，その期待値を求めることで，参加の損得を考える．
ある時刻$t$において，エージェントが予測する価値を$Q_t$，引いたカード(報酬)を$r_t$とすると，
以下の式から次の予想値を計算する
$$
Q_{t+1} = Q_t + \alpha (r_t - Q_t)
$$
$r_t - Q_t$は予想との誤差を表し，$\alpha$は誤差を考慮して，どれだけ予想値を一度に変更するかの正パラメータである．

詳細は，GambleTask.ipynbを参考にする．


#### 行動選択を含める
選択肢が複数ある場合を考える．

2腕バンディット課題: 2つのケースがあり，どちらを選択するかを決められる．

予想した価値$Q_t(A), Q_t(B)$を考えて，価値が高い方を高確率で選択しつつ，価値が低い方も一定の確率で選択するように考えると，

$$
P(a_{t+1} = A) \propto exp(\beta Q_t(A)))　\\
P(a_{t+1} = B) \propto exp(\beta Q_t(B)))
$$
ここで，$a_t, \beta$はそれぞれ，とる選択肢，現時点で$Q$の値をどれくらい信用するかを決めるパラメータである．
このような式をソフトマックス(softmax function)といい，この確率に従って毎回A, Bを選択する．

一度に得られる情報はA, Bのどちらか一方なので，価値の更新も片方のみである．
このように価値を更新していく学習の仕方を，Q学習(Q learning)という．

実際の解析はTwoBandit.ipynbに行う．


#### モデルのバリエーションと発展
アスピレーション学習(aspiration learning): ある基準(アスピレーション)を設け，ある行動をした時に得られた報酬がそれよりも大きいかどうかに応じて，その行動をとる確率を直接上下させる方法．

また追加ルールがある場合は，それに対応した変数を入れて，価値の種類を増やして，モデル化を行えばよい．

#### 行動モデルとして使う
これらのモデルは時系列モデルと捉えることもできる．
よって，実際の行動時系列データにフィッティングすることで，パラメータを推定したり，予測を行うことができる．
モデルのパラメータには直接的な意味があるので，推定されたパラメータを使って，その行動の背後にある原理に潰え推測することもよく行われる．

今回の節では，学習が進むプロセスそのものに興味がある．
機械学習の文脈における強化学習は，学習された最適な戦略に興味がある．
しかし，背後には共通した考え方，モデル構造がある．

---
### 9.2 機械学習としての強化学習
#### 機械学習としての強化学習
機械学習の文脈での強化学習は，状況に応じてそれぞれの行動の価値を正しく決めることが目標．
環境が複雑になると，Q学習では，環境の状態のバリエーションが膨大になるなどの問題点が生じる．
これを解決するために様々な工夫がされている．

#### 価値観数の性質を決める
システムの状態$s$における行動$a$の価値を$Q(s, a)$とすると，すべての状態．行動について価値$Q$がわかれば，高得点を出すための操作を行うことができる．

次に，ある行動$a$をした時にシステムが$s'$という状態にあるとして，この時に得られる報酬を$r(s, a)$とする．
最終的に得られる総得点を最大化するなら，この後に得られる報酬も考慮に入れて，次の行動を選択すべき．
つまり以下の関係式を仮定する(実際は期待値が入る)．
$$
Q(s, a) = r(s, a) + \gamma Q(s', a^*)
$$
これは状態$s$の時の行動$a$の価値 = 状態$s$の時の行動$a$で得られる報酬 + 引き起こされる状況$s'$の最大価値×割引率

#### 価値観数の更新
$$
Q(s, a) \leftarrow Q(s, a) + \alpha [r(s, a) + \gamma Q(s', a^*) - Q(s, a)]
$$

右辺の$\alpha$の中は，$Q(x, a)$のあるべき値と現在の値の差になっていて，このように差の値を使って$Q$の値を更新していく方法をTD学習(temporal-difference learning)という・

#### 深層学習を使ったQ学習
実際に全ての場合についての$Q(s, a)$を更新していくのは現実ではない．
そこでニューラルネットワークを使って，この関数を近似するDQN(deep Q network)という方法がある．

#### Q学習以外の方法
Q学習の方法を価値ベース(value base)の強化学習という．
それ以外に，どのような戦略で行動を選択していくかを学習する方策ベース(policy base)の強化学習もある．

--- 
### 第9章のまとめ
- 強化学習は，未知の環境の状況に応じて意思決定する主体(エージェント)が適切な行動を学習する様子をモデル化する
- 強化学習は，人間の学習行動をモデル化する時系列モデルとして利用される
- 強化学習は，機械学習の文脈で最適な行動を機械に学習させる方法としても利用される

---
---
## 第10章 多体系モデル・エージェントベースモデル
複雑なシステムを見せるシステムでは，個々の要素の集まりが本質的な役割を果たしている．
個々の要素がたくさん集まった時に，全体として見える振る舞いを分析するための手法が，多体系モデル・エージェントモデルである．

### 10.1 ミクロからマクロへ
多体系モデル(many-body system)，エージェントベースモデル(agent-based model):
個々の物体・エージェントの振る舞いを記述するモデルをたくさん用意し，互いに相互作用させたもの．
ミクロな現象と全体として発生するマクロな現象の間のギャップを埋めたり，ミクロな振る舞いがわかっている状態で，マクロな現象がどのようになるかを予想したい時に用いる．

#### モデルの構成要素
多体系モデルの本質は要素同士がどのように相互作用しているかを記述すること．
要素の振る舞いを記述するのに常微分方程式モデルや確率過程モデルを利用する．

代表的な相互作用の仕方

- それぞれの要素が全ての要素と，同じ強さで相互作用する
    - well-mixed populationという．数理的に解析することが容易な場合が多い
- 要素が二次元や三次元空間の「位置情報」をもっていて，近くの要素だけ相互作用する
    - モデルに空間的な情報が含まれている時，遠くの要素とは相互作用しないという仮定．要素が動くと相互作用の相手も変化する
- 要素の間に特定のつながり(ネットワーク)が定められていて，繋がっている相手とだけ相互作用する
    - ネットワークとは，要素同士の繋がり方の情報をまとめたもの．つながりの一つ一つのことをリンク，個々の要素をノードという

#### 時間・空間の離散化
多数の要素の数理モデルを同時に動かすため，理論解析が難しくなるだけでなく，シミュレーションの計算コストが莫大になることがある．
そのため，時間や空間を離散的に区切り，単純化する離散化(discretization)と行う．
離散化された時間を時間ステップ(time step)，空間の1マスをセル(cell)やサイト(site)という．

セルオートマトン(cellular automaton): 時間と空間を離散化した決定論的なシステム．

時間を離散化すると，それぞれの要素をどの順番で更新して動かすかを指定する必要がある．

- ランダムアップデート(random update): ランダムに各要素を選んで更新する
- 逐次アップデート(sequential updata): 決まった順番に更新する
- パラレル・アップデート(parallel update): 1ステップにすべての要素を同時に更新して動かす

離散化は便利な単純化であるが，現象の本質を損なう可能性もあるので，気をつける．

#### マクロな変数によってシステムの振る舞いを特徴づける
秩序変数(order parameter): システムの状態を特徴づけるマクロな変数．
パラメータの変化によって秩序変数が急激に変化して，システムが異なる状態に変化することを相転移(phase transition)という．

#### モデルの分析の仕方
1. 理論解析
    - 相互作用や個々の要素の振る舞いが完全に同じルールに従っている場合，理論解析が可能な場合がある．
2. シミュレーション解析
    - 数値実験し，その結果から理解を深めたり，現象を予測したりする．シミュレーション中に起きたことは全て測定可能なので様々な角度から結果の検討が可能
    
---
### 10.2 さまざまな集団モデル
#### 群れのモデル
複数の個体で群れをなして移動する鳥や魚の群れはどのようにして作られ，維持されているのか．

Vicsekモデル: 個体が二次元の平面状を移動していき，すべての粒子は同じ速さで動き，周りの状況に合わせて方向だけ変化させる．
$$
\theta(t + \Delta t) = <\theta(t)>_r + \epsilon
$$
$\theta(t)$はある時刻の方向，$<\theta(t)>_r$は周りの粒子の平均方向，$\epsilon$はノイズ．
各時間ステップでは，速度の向きに応じて位置を動かして更新し，全ての粒子に対して，パラレルに実施する．

#### 同期現象のモデル
同期現象(sychronization): 何かの周期的な動きのタイミングが一致すること．

蔵元モデル(Kuramoto model): 同期現象を理解するための代表的なモデル．
ある要素の振動を位相(phase)という変数の動きで表現する．
それぞれの要素(振動子; oscillator)は自身の位相の動きを持っているが，他の振動子との位相の差に応じて自身の位相を調整する．
相互作用が大きくなると，徐々に位相の動きがあってきて，最終的に全体として同期する．

#### 人間行動・意思決定のモデル
社会的ジレンマ(social dilemma): 全員が合理的な行動をとると最終的に全員が損する問題．
このような状況における人々の行動は社会科学の諸分野で研究されてきた．

囚人のジレンマゲーム(prisoner's dilemma): 「協力」か「裏切り」を選択でき，それぞれのパターンに応じて得点が与えられる．
このゲームを繰り返しプレイさせた時の様子を分析することで，人間社会における協力現象をモデル化することがよく行われる．
各プレイヤーに行動を選択させ，その行動に基づいて得点を与え，その得点をもとに行動をアップデーして次の行動をとる．

この行動のアップデートを行う方法は二つある．

1. 進化論的アプローチ
    - 自分が対戦した相手が得た得点を参照して，一番得点が高いプレイヤーの取った行動を真似する．
2. 強化学習的アプローチ
    - それぞれの行動の価値を，毎回のゲームに応じてアップデートしていく．
    
---
### 10.3 相互作用のネットワーク
#### ネットワーク構造で問題を眺める
複雑ネットワーク科学(complex network science): ネットワークそのものを調べることで，システムの特徴を説明する．

#### どれくらい他のノードとつながっているか
次数(degree): あるノードについて，繋がっているノードの数．$k$とかく．
この次数を全てのノードについて調べ，次数の出現分布$P(k)$を次数分布(degree distribution)という．
世の中のネットワークはこの次数分布がべき分布($P(k)\propto k^{-\gamma}$)になっていることがある．
このようなネットワークをスケールフリーネットワークといい，その上でのダイナミクスの伝播が早まったり，リンクの多いノード(ハブ; hub)の影響力が支配的になったりする．

#### 類は友を呼ぶ？
次数が$k$のノードと繋がっているノードたちの字数はどのくらいか？という指標が重要になることもある．

リッチクラブ(rich club): 次数の高いノード同士が互いに繋がった構造．

アソータティビティ(assortativity): 同じ次数のノード同士がどれくらい繋がりやすいかを表す指標．

#### ネットワーク上の移動のしやすさを特徴づける
最短経路長(minimum path length): あるノードから別のノードに最短経路で移動する際に通らなければならないリンクの数．

平均経路長(average path length): この最短経路長をすべてのノードのペアに対して計算し，平均したもの，
ネットワーク上の移動のしやすさを表す指標．
現実のネットワークでは，想像に反して短いことが多く，空港(全世界で4000ある)の平均経路長は約3である．

#### 「中心性」で重要なノードを特徴づける
- 媒介中心性(betweenness centrality): 平均経路長を求めた際に，それらの経路が着目しているノードの上を通過した割合のこと．
- 次数中心性(degree cetrality): 次数の大きさだけをみる
- 近接中申請(closeness centrality): 他のノードとの距離をみる

#### 「友達の友達」は友達か
クラスター係数(clustering coefficient): 三角形の関係がネットワーク中にどれくらいあるかを計算した値．
クラスター係数が高いほど，ノードがグループとしてまとまっているといえる．

コミュニティ構造(community structure): クラスター係数が高いグループ同士が，少ない数のリンクで繋がっている構造．
このようなネットワーク上では，コミュニティの中と外でダイナミクスの伝播の仕方が異なることがある．

#### ランダムなネットワークを作る
Erdős–Rényiモデル: ランダムネットワークの生成モデル．ベースラインの比較対象として利用される．

#### スケールフリーネットワークの基本モデル
Barabási–Albertモデル: スケールフリー性をもったネットワーク．
リンクをたくさん持っているノードが新たにリンクを得やすい優先的選択(preferential attachment)というルールがある．

#### コンフィギュレーションモデル
コンフィギュレーションモデル(configuration model): 指定した次数分布を持ったランダムなネットワークを生成する．

---
### 第10章のまとめ
- 個々の要素の振る舞いから，それらが相互作用して全体としてどのような振る舞いを示すかを調べるモデルを，多体系モデル・エージェントベースモデルという．
- 個々の要素のモデルには，微分方程式モデルや確率モデル，強化学習モデルなど問題に応じて適切なものを用いることができる
- 相互作用の仕方を決めるネットワークの構造を調べることで，全体のダイナミクスについての示唆を得ることができる

---
---
## 第三部のまとめ
様々なモデルについて基礎的な内容を説明した．
何かが時間変化する様子をモデル化する，という一つの課題に対して，複数のアプローチが可能である．
そうした選択肢の中から実際にどうやってモデルを選ぶのか，それをどのように活用するのかは第四部で解説．


---
---
---

# 第四部 数理モデルを作る
実際に数理モデルを利用する時に必要な，目的に応じたモデルの利用法の違い，アプローチの選定，パラメータの推定，モデルの評価について説明する．
さらに，データ取得時の注意点やモデル構築のノウハウ，数理モデルによって導かれた結論が何を意味するのかも説明し，数理モデルがしっかり力を発揮するための理解しておきたい内容を紹介する．

---
## 第11章 モデルを決めるための要素
今分析したい対象に対して，実際にどのような数理モデルを選べば良いか？
データの性質や問題に応じて，どのような点に注意してモデルを選択すべきかについて説明する．

### 11.1 数理モデルの性質
#### 数理モデルの目的
理解志向型モデリングか応用志向型モデリングかを決めて，目的を期待することが重要．

#### モデリングは試行錯誤
大きく分けて以下のステップからなる．

0. データを可視化する
1. 問題・目的を定義する
2. どのモデルを使用するか決める
3. パラメータを推定する
4. モデルの性能を評価する

これを何度も繰り返して試行錯誤し，場合によっては問題設定から見直すことも必要である．
場合によっては，数理モデルを使わなくても，ルールベースアルゴリズムで解決できることもある．

#### 決定論的モデル vs 確率モデル・統計モデル
数理モデルは，決定論的モデルと確率モデルに大別できる．
決定論的モデルは，確率の概念が入っていないモデルで，データの訂正的な振る舞いや平均的な振る舞いの理解を深めたい，ノイズが無視できる場合は有効である．
通常のデータは様々な要因によってばらついているため，確率モデルを利用すべきである．

#### 利用可能なモデルの検討
既存モデルをよく吟味した上で目標が達成できない場合，問題点を特定して新しいモデルを作成する．

---
### 11.2 理解志向型モデリングのポイント
#### 理解しやすいモデルとは
理解志向型モデリングにおける最終的な目標は，現象・データ生成ルールの理解．
以下の条件を満たすと理解しやすいモデルと言える．

- パラメータの数が少ない
- 使用している関数が簡単
- モデルの各要素(数理構造・変数・パラメータ)が直感的に理解できる
- 数理的に解析できる

パラメータの数が多かったり，関数が複雑だとたまたま整合しただけなのか，本質的に何かを捉えているのかを判断するのが困難になる．
モデルに含まれている全ての要素を言葉で説明する必要がある．
**数理モデルから得られた説明の強さは，数理モデルにおける論理が一番弱い部部と同じ強さもしくはそれ以下**である．

#### 簡単なモデルなら何でもいいわけではない
「データをどれくらい説明できるか」と「モデルの複雑さ」はトレードオフの関係．
データへの当てはまりが同程度なら，基本的に理解しやすいモデルを選択する．

#### 理解したい深さとモデリング手法
現象のメカニズムには階層があり，どのレベルでの理解が求められるかによって使用するべきモデルが異なる．
理解したい・説明したいレベルで数理的な記述をモデルに含める．

#### 数理モデルと演繹
数理モデルによって，変数たちの関係性やダイナミクスについてがよく説明できた時，まず推論できるのは，
その変数たちはそのように動いている．ということ．
さらに，そのモデルが正しいという過程のもで，演繹的に様々なことを示したり，予測することができ，
それらが論理的に正しい手順で行われれば，演繹によって得られた結果の信頼性はモデルの信頼性と一致する．

#### モデルで指定したメカニズムのレベルよりも根源的なことは説明できない
「なぜ仮定したダイナミクスが生じているのか」という，数理モデルのメカニズムが生じるメカニズムを説明することはできない．
それを言いたいなら，さらにその階層でモデリングを行う必要がある．

例えば，対象となるデータの統計分布を統計モデルで表現した場合，データがそのモデルに従っていること，何が起こるかを計算することは出来るが，
なぜその分布が出てきたのかについては何も言えない．
対象の振る舞いを直接モデリングした結果，同じ統計分布が出てくることを説明できれば，その仮定した振る舞いからその分布が出てくるメカニズムを理解することができる．

**どのレベルでデータとモデルを合わせるか**は非常に重要な視点である．

---
### 11.3 応用志向型モデリングのポイント
#### 問題を定義する
達成したい目標を数値で表現する．

例えば，「◯◯の分類問題で，入力データは△△で出力データは××のラベル，そのラベルの予測の正解率を指標とする」

しかしながら，一見もっともらしい評価指標も実際にモデルを動かすと実用上求められているものとは異なった，ということがしばしば起きる．
また運用上のコストについても考える．
継続的にデータを取得して，モデルをアップデートする必要があるのか，など．

#### 性能を重視したモデル選び
モデルの決定は性能の良し悪しで評価する．性能を評価する指標は様々あるが，「モデル選択」が重要である．

#### データの性質
どのようなデータを利用することができるかも重要である．
数理モデルは現実世界の現象を直接再現するのではなく，あくまでも与えられたデータの生成ルールを再現するので，
データが偏っていたり，誤差や欠損値が多く含まれていると，モデルの性能にそのまま反映される．
またデータの変数が不足して，手がかりとなる情報が足りないこともよくある．

---
### 第11章のまとめ
- 数理モデルを決めるために，まず目的と使用できるデータを吟味する
- 対象となるデータの性質も，使用するモデルを決める重要な要素である
- 理解志向型モデリングでは，達成したい理解のレベルに応じてモデルを決める
- 応用志向型モデリングでは，真に達成すべき目標を正しく評価指標に落とし込むことが必要

---
---
## 第12章 モデルを設計する
数理モデルを使った問題設定ができたとして，次にどの変数を含めるか，どのような要素を数理構造の中に配置するか，どこにパラメータを用意するかを問題に応じて決定する必要がある．

### 12.1 変数の選択
#### 含めるべき変数・そうでない変数
数理モデルの性能が変わらないなら，変数は少ないほどよい．

次元の呪い(the curse of dimensionality): 変数が多いほど，モデルの解釈性が下がったり，パラメータ推定のコストや過学習の危険性が増大すること．

しかし，モデルの性能向上のために必要な変数をいれなければならないので，どの変数を入れるかが非常に重要なポイントになる．

#### 変数の解釈性
理解志向型モデリングの場合，現実に何に対応するか説明できない変数は，出来るだけ排除する．
そのような変数があると，モデルを使って現象のメカニズムを説明する際に，論理的な演繹が出来なくなるから．

#### 無関係な変数は外す
対象のデータ生成規則に関係のない変数はモデルに含めない(ID番号など，リーケージ(IDに実験条件の情報が載ってしまうこと)につながる．)．
また理解志向型モデリングの場合，本質的に同じ情報を表している変数は次元削減するか，代表的なもの以外を除外する方がよい．

一方で，明らかに重要な他と独立した変数は，分析の結果として除いても影響がなかったとしても，一度はモデルに含めた方が良い場合がある．
それにより，「関係があると思ってモデルに含めたけど，結果として関係なかった」と結論づけることが可能．

#### 特徴量エンジニアリング
特徴量エンジニアリング(feature engineering): モデルの性能が良くなるように既にある変数を組み合わせて新たな変数を作り出すこと．
理解志向型モデリングの場合は，モデルの解釈性が下がったり，統計検定におけるp-hackingに繋がるため推奨されない．

#### 離散値変数・連続値変数
離散値変数を使ったモデルの特徴

- 値の表現の幅が離散であるため，表現に不正確性が生じる
- パターンの数が数えられるので状態の数が減り，扱いやすくなることがある
- 変数に関する微積分が行えないため，論理的な解析・パラメータの推定が難しくなる

連続値変数を使ったモデルの特徴

- 離散化による誤差なしで値を表現できる
- モデルのとりうる状態の数が数えられなくなり，扱いにくくなることがある
- 変数に関する微積分が行えるため，一般的に理論的な解析・パラメータ推定がしやすい

### 12.2 データの取得・実験計画
#### 着目する変数の影響をコントロールしながらデータを取得する
数理モデルの性能はデータの質に大きく左右される．

実験計画法(design of experiments): 対象について様々な要因が考えられる状況で，どのようにデータ取得をデザインするか．
ありうる条件の組み合わせの内，どれを何回どのような順番，まとめ方で実施するかを検討する．
そして分散分析(analysis of variance; ANOVA)という統計的な手法を用いて各要因が与える影響を評価する．

統計解析でない数理モデル分析を行う際にも有用である．

#### フィッシャーの三原則
フィッシャーの三原則: 着目している要因以外から生じるデータの偏りをコントロールする．

- 反復(replication): 同じ条件で観測を繰り返す．平均値としてより信頼できる値が求まり，さらに測定誤差の大きさを見積もることができる．
- 無作為化(randomization): 観測行う場所や順番などをランダムに決めること．着目している要因以外の条件を出来るだけ均一にする．
- 局所管理(local control): 無視できない要因の影響をコントロールすること．

#### フィッシャーの三原則はデータの偏りに気を付けるためのヒント
分散分析を行わない場合でも，取得したデータが結果に影響を与えるかもしれない要因がしっかりコントロールされているかどうかを検討することは重要である．
フィッシャーの三原則が満たされていない場合は，どれが満たされていないのか，それによってどういった偏りが生じうるかをチェックすることでより精度の高い分析が可能になる．

### 12.3 数理構造・パラメータの選択
#### 目的に応じた数理構造の選択 
応用志向型モデリングの場合は，基本的な分析であれば，チャートに従って機械学習モデルを選択することができる．
一方で理解志向型モデリングの場合，問題に応じてそもそもどの種類のモデルを使用するかを決める必要がある．

#### 目的変数のばらつきが無視できない場合
まず目的変数の振る舞いにおいて，確率的なばらつきが本質的か無視できるかを考える

- ばらつきが本質的な場合
数理モデルは目的変数の確率的な振る舞いを再現することを目指す．
ばらつきが大きくない場合は，値を確率的に一定の精度で予測することは可能だが，ばらつきが大きい場合は個々の値の予測は難しく，背後にある確率分布の形からメカニズムを推測する．
また，なぜその分布が出てくるのかを調べたい(メカニズムを記述して説明したい)場合は確率モデルを利用する．

#### ばらつきを考えなくて良い場合
目的変数を説明変数で表した関数を求めることが目標．
関数が生じているメカニズムを知りたい場合は，決定論的な数理構造(常微分方程式やセルオートマトン)などを用いて，変数の振る舞いを記述する．
データが行う方程式の形がすでに分かっている場合はカーブフィッティングを行う．

#### パラメータの値の範囲
変数と数理構造を決めるとパラメータが必要な場所が自然と決定する．
パラメータの値に意味がある場合，その範囲に気を配る必要がある(正負など)．

### 12.4 間違ったモデリングをしないために
#### 既存の体系との整合性・比較
新しいモデルを作ったり，既存のモデルを拡張する場合，従うべき既存の体系・法則と整合するようにする．
現実と乖離した振る舞いがモデルに含まれていると，現象を説明する論理が破綻してしまう．
既存の体型で説明できないものをモデル化する時には，提案モデルで既存体系の何が破られているのかを明確化する必要がある．

#### ハンマーしか持っていない人にはすべて釘に見える
If all you have is a hammer, everything looks like a nail.

問題を見た時に，自分が使える限られたモデリング手法の問題として，無意識的に解釈してしまう．
数多のモデリング手法の中から，最も性能が良いものを選択して使用するべきである．

そもそもモデリング手法を知らないと調べようがないので，本書でモデリング手法を大まかに示している．

#### データは適切に前処理しておく
取得したデータは適切に前処理して数理モデルに適用する．
前処理の仕方は，結論やモデルの性能に大きく影響を与える

- 外れ値(outlier)の処理: 外れ値とは，他の値とは大きく異なる値のこと．論理的に確実な方法はない．外れ値によって誤った結論が導かれることがあるので，注意する
    - 判断基準がある場合は，それを使用する．
    - 統計検定を用いて外れ値を特定する．
    - 外れ値があっても大きく影響されない分析手法を用いる
    - 外れ値を入れた分析と除いた分析の両方の結果を報告する

- 欠損値(missing value)の処理: データにおいてそもそも値が抜けてしまっている値のこと．欠損発生の意味をまず調べる．
    - データからその点を取り除く
    - 適当な値(平均値，中央値)でうめる．あまり推奨されない？

---
### 第12章のまとめ
- 理解志向型モデリングでは，必要な変数を吟味して使用する
- 応用志向型モデリングでは，少しでも使える情報は使う
- 理解志向型モデリングの数理構造は説明したいデータのばらつきが本質的か・無視できるか，また説明メカニズムのレベルで選ぶ
- モデルと現実・既存体系との整合性を確保しつつ，一番適切なアプローチを選択する
- 外れ値や欠損値，その他のデータの質が数理モデリングの質を決める

---
---
## 第13章 パラメータを推定する
数理モデルが出来たら，データによく当てはまるようにパラメータを推定する．
パラメータの決め方に，モデルや問題設定によって様々な方法がある

### 13.1 目的に応じたパラメータ推定
#### 動かせるパラメータと動かせないパラメータ
パラメータの値が実験などですでに求まっている場合に，違う値を入れても現実と乖離したモデルになる(あえて違う値をいれることで本質を知ることができるが)．
ボトムアップ的な理解志向型モデリングの場合は，全てのパラメータが大体わかっていて自動的に決められるのが理想．

#### パラメータの点推定
点推定(point estimation): パラメータの値の組を一つに決めること．

#### 変数の振る舞いを定量的にデータと合わせたい場合
目的関数(objective function): モデルから生成される値と実際のデータとの差(誤差)を計算するための指標．
平均二乗誤差(mean squared error; MSE)や対数尤度(log likelihood)などさまざまある．

#### 単に誤差の大きさを平均する
平均二乗誤差や外れ値から受ける影響を弱めた平均絶対誤差(mean absolute error; MAE)などがある．
また，誤差が小さいデータには二乗誤差，大きいデータには絶対誤差を計算して和をとるHuber損失関数(Huber loss function)や，
値が一定以内に収まっていれば誤差を0とすることで過学習を防ぐ$\epsilon-$許容損失関数($\epsilon-$insensitive loss function)などがある．

#### 対数尤度
モデルが確率的な要素を含んでいて，あるデータが得られる確率を直接記述できる場合，対数尤度によってモデルの当てはまり具合を評価できる．
尤度とは，仮定されたモデルに，全ての観測値がそのモデルから出現する確率を表していて，尤度が大きいほどモデルがよくデータを表現していることになる．
尤度を数式で書くと
$$
L = p(X|\theta)
$$
であり，$X$はすべての観測地，$\theta$はパラメータである．
実際には，計算しやすくするために対数をとって行う．

このように，尤度を目的関数としてパラメータを推定する方法を最尤推定(maximum likelihood estimation; MLE)という．

しかし，尤度を最大化するパラメータが真のパラメータの良い近似を与えることは自明ではなく，漸近正規性が担保されないモデルでは理論的な保証が失われる．

#### 確率分布間の「差」を最小化する指標
カルバック・ライブラー情報量(Kullback-Leibler divergence): 分布間の差を定量化する指標．

#### 交差エントロピー
交差エントロピー(cross entropy): 情報量の観点から二つの分布の近さを定量化する

---
### 13.2 パラメータ推定における目的関数の最小化
#### 目的関数を最小化するには
適切に設定した目的関数を最小化するパラメータをどうやって求めるのか

#### 解析的に解く
データとモデルが与えられた時の目的関数$L$は，パラメータ$\theta$の関数になるので，
$$
\dfrac{\partial L}{\partial \theta} = 0
$$
となるパラメータ$\theta$を求めれば良い．

#### パラメータをスウィープする
解析的に求められない場合に，パラメータに数値を入れて目的関数を計算する方法がある．

パラメータスウィープ(parameter sweep): パラメータの値を試す方法．
調整するパラメータの数が多ければ使えないが，正しく大域最適解を探せる可能性がある．
またある程度，最適解の目星をつけた後に，二分法や最急降下法でパラメータを求めることもある．

#### 最急降下法
最急降下法(gradient-descent method): 目的関数が具体的に数式で計算でき，そのパラメータによる微分が計算できる場合に用いる．
$$
\theta \leftarrow \theta - \alpha \dfrac{\partial L}{\partial \theta}
$$
を使って，パラメータの値を更新し，変化しなくなるまで行う．

#### 局所解陥らないために
初期値をランダムに設定したり，確率的勾配降下法(stochastic gradient descent)などを用いる．

確率的勾配効果法は，最急降下法でパラメータを更新する際に，データを全て使わずに毎回一部のデータをランダムに用いる方法．
局所解から抜け出しやすくなる他，計算コストを軽減できることもある．

一方で，大域最適解を求めることはほぼ不可能なので，実用上は性能が良い局所解で良い場面もある．

#### 過学習を防ぐ
過学習を防ぐために，データをほどほどに信用して合せすぎないようにする方法がある．

正則化(regularization): 目的関数にパラメータの「値の大きさ: ノルム」である$||\theta||$を足した$L(\theta) + \lambda ||\theta||$を最小化する．
このノルムの定義にはいくつか方法がある．

- L2ノルム正則化: モデルに含まれる全てのパラメータの値を二乗して足し算する．$||\theta|| = \Sigma_{i} {\theta_{i}}^2$
- L1ノルム正則化: パラメータの値の絶対値を足し合わせたもの．$||\theta|| = \Sigma_{i} |\theta_{i}|$

L1ノルム正則化の方が，値が小さいパラメータに対する罰則が強いので，値が0になるパラメータの数が増える．
つまり，パラメータの数を減らしてモデルの推定を行うことになる．
これをスパースモデル(sparse model)という．

正則化がうまくいくかは，データによるので試行錯誤が必要である．

#### 目的関数最小化の実施
モデルに応じた計算ライブラリがあり，簡単に目的関数の最適化によるパラメータ推定が可能である．
モデルが複雑な場合は，MCMCによる手法を行う．

微分方程式モデルや，複雑は確率モデル，多体系・エージェントベースモデルにおいては，このようなライブラリが存在しないが，
これらは理解志向型モデリングとして採用されることが多く，目的関数最小化によるパラメータ推定を行うニーズがほとんどない．
(理解志向型モデリングでは，パラメータは最初から自動的に決まることが理想だから)

最後に，
**定量的に十分な予測力をもたないモデルにおいて，パラメータの値を細かくきっちり決める行為には意味がない．**


---
### 13.3 ベイズ推定・ベイズモデリング
#### パラメータの分布を考えるのがベイズ推定
ベイズ推定(Bayesian inference): データからパラメータの確率分布を推定する方法の一つ．

#### パラメータの確率分布？
事前分布$\phi (\theta)$，パラメータ$\theta$が与えられた時の$X$の確率密度関数を$p(X|\theta)$とする．
データ$X^n$が与えられた時のパラメータの条件付き密度関数は，
$$
p(\theta|X^n) = (1/Z) \phi(\theta)p(X^n|\theta)
$$
で表され，この$p(\theta|X^n)$を事後分布(posterior distribution)という．
ただし$Z$は正規化定数であり，詳細は省く．
この事後分布から以下のように予測分布$p(x|X^n)$を計算することをベイズ推定という．
$$
p(x|X^n) = E_{p(\theta|X^n)}[p(x|\theta)] = \int{p(x|\theta)p(\theta|X^n)}d\theta
$$

#### 推定された分布を特徴づける
パラメータの値を一つに決めたい時は，事後確率が最大になるMAP estimatorや，事後分布による期待値EAP estimator，中央値(MED estimator)などを計算して点推定値とする．
また分布の標準偏差(事後標準偏差)を求めれば，パラメータがどれくらいばらついているかを特徴づけられる．

#### マルコフ連鎖モンテカルロ法
事後分布を求める際に，解析的に計算を行うことは困難なことが多いので，マルコフ連鎖モンテカルロ法(Markov chain Monte Carlo; MCMC)を用いることが多い．
簡単な原理としては

数値的に求めたい確率分布を$q(\theta)$とすると，まず確率変数の従う確率モデルを考える．
このモデルをシミュレートして動かすと，最終的に$(\theta_1, \theta_2, ..., \theta_t)$が得られ，この出現確率の分布が求めたい確率分布$q(\tehta)$と一致することができる

#### メトロポリス法
MCMCにおいて，具体的に確率過程を求める方法の一つがメトロポリス法．

1. $\theta$の初期値をランダムに決める
2. 現時点での値$\theta_t$からランダムに値を変化させた値$\theta_t'$を用意する
3. 関数の値の比$q(\theta_t')/q(\theta_t)$を計算する
4. この値が1より大きければ$\theta_t'$を採用し，小さい場合は，確率$q(\theta_t')/q(\theta_t)$で$\theta_t'$を採用し，残りの確率で元の値を維持する．
5. 2-4を繰り返す

十分長くシミュレーションすれば，初期条件によらない定常分布が得られる．

---
### 第13章のまとめ
- モデルを定量的にデータと細かく一致させたい場合には，平均二乗誤差や対数尤度などを目的関数として，それを最小化するパラメータの値を求める
- 目的関数の最小化には，幅広い問題に使える最急降下法などの手法を用いる
- パラメータを1つの値ではなく分布と考えるベイズ推定の考え方も非常に有用で，推定されるパラメータ分布の様々な情報を利用することができる

---
---
## 第14章 モデルを評価する
数理モデルを作成する際に，試行錯誤をするが，モデルを選択するための指標が必要になる．

### 14.1 「良いモデル」とは
#### 目的に応じたモデルの評価
データによく当てはまっていればそれだけで良いわけではない．
モデルの良さを評価するための考え方・指標も目的に応じて変わる．

#### メカニズム理解を目的としたモデルの評価
- モデルの解釈性: モデルの各要素(変数・数理構造・パラメータ)がすべて説明可能で，既存体系と矛盾していないか．まt対象のデータ生成規則を再現する最小限の構成になっているか
- 当てはまりの良さ: モデルがデータと許容される範囲で当てはまり，整合的か．

#### 統計的推論を行うためのモデルの評価
解釈性とデータへの当てはまりのバランスが大事だが，よりデータへの当てはまりの重要度が高まる．
モデルの複雑さとデータへの当てはまりの良さのバランスを数値的に評価する方法論がある．

#### 応用志向型モデリングにおけるモデルの評価
解釈性はそこまで重視されず，未知のデータへの当てはまり(予測性能)が良いかが重要になる．

---
### 14.2 分類精度の指標
#### 当てはまりの良さ・性能を測る
適合度(goodness of fit): 当てはまりの良さのこと．目的関数や決定係数などもそうである．

#### 正解率・再現率・特異度・適合率・F値
混同行列(confusion matrix)

|  | 実際は陰性 | 実際は陽性 |
| --------  | ---------- | -------- |
| 陰性と予測 | 真陰性(TN)| 偽陰性(FN) |
| 陽性と予測 | 偽陽性(FP) | 真陽性(TP) |

- 正解率(accuracy): 実際に予測された値が正解する割合．$\dfrac{TN+TP}{TN+FN+FP+TP}$
- 再現率(recall): 陽性をの人を正しく陽性だと当てた割合．$\dfrac{TP}{FN+TP}$
- 特異度(specificity): 陰性の人を正しく陰性と当てた割合．$\dfrac{TN}{TN+FP}$
- 精度(precision): 陽性と予測した人の中で実際に陽性であった割合．$\dfrac{TP}{(FP+TP)}$
- F値(F score): 精度と再現率の調和平均．

目的に応じて，それぞれの指標を使い分ける．

#### ROC曲線とAUC
実際のモデルでは，どのクラスに属するかを0-1の間の数値で予測する．この時の基準を閾値(threshold)といい，
理想的なモデルではこの閾値を正しくとれば全てに対して正しく分類できるし，反対に，性能の低いモデルでは閾値をどんな帯にしても一定割合で誤った分類をしてしまう．

ROC曲線(receiver operating characteristic curve): 閾値を変化させることによって，モデルが予測する分類スコアがどれくらい2つのクラスで分離しているかを評価する指標．
縦軸に再現率，横軸に偽陽性率(=1-特異度)をとり，プロットしていく．
そのグラフの下部面積をAUC(area under the curve)といい，モデルの評価指標として利用できる．
1に近づくほど良いモデルである．

---
### 14.3 情報量基準
#### モデルが複雑ならば適合度は上がる
モデルを複雑にすれば，過学習の危険性も高まる．
よって，モデルを適切な複雑さにとどめつつ，その中で出来るだけよくデータを説明するモデルを選ぶ必要がある．

情報量基準(information criterion)という指標が一般的に用いられる．
情報量基準は同じ規則で生成された未知のデータをどれだけ説明できるかというアイデアに基づいている．

#### 赤池情報量基準AIC
赤池情報量基準(AIC)
$$
AIC = -2\ln{L} + 2k
$$
ただし，$L$はモデルの最大尤度(パラメータ推定して尤度を最大化したもの)，$k$はモデルに含まれている自由に動かせるパラメータの数．

#### ベイズ情報量基準BIC
$$
BIC = -2\ln{L} + 2k\ln{n}
$$
ただし，$L$はモデルの最大尤度(パラメータ推定して尤度を最大化したもの)，$k$はモデルに含まれている自由に動かせるパラメータの数，$n$はデータの観測数．

#### その他の情報量基準
- 最小記述長(MDL)
- 逸脱度情報量基準(DIC)
- WAIC, WBIC

一般的に，このような指標に基づいたモデル選択でも，「真のモデルは分からない状態で限られたデータからモデルを評価する必要がある」という困難から逃れられない．
作ったモデルが未知のデータに対してよく機能することを実際に確かめることができれば，それが一番確実である．
テストデータを用意できない場合には，情報量基準が役に立つ．

### 14.4 ヌルモデルとの比較・尤度比検定
#### モデルに入れた要素に意味があるか
あるモデルが別のモデルに含まれている(ネストされている; nested)場合に，これらを比較することを考える．
この分析はより適合度の良いモデルを探したり，対象となるシステムにおいてある要素が重要であるかどうかを検証できる．
二つのモデル，提案したいモデルとヌルモデル(null model; 主張に必要な要素を除いたモデル)を比較する．

提案モデルの方がデータをよく説明することを示せば，この要素は重要なファクターであるという主張がいえる．

#### 尤度比検定
尤度比検定(likelihood): データがヌルモデルから生成されていると仮定し，ランダムにデータを生成して，ヌルモデルと提案モデルの適合度の差をみる．
この手続きを何度も行い，適合度の差の分布を作り，その分布の中で実際のデータに当てはめた際に得られる適合度の差が発生する確率を計算する．
それが事前に決めた有意水準よりも小さければ，有意に「良いモデル」であると主張できる．

#### 統計検定とヌルモデル
モデル間の比較の文脈でヌルモデルの利用することは，統計検定の文脈で自然と行われている．
データからある量を計算したところ，ヌルモデルでは説明できない異常な値が出ているという主張を行う時，このヌルモデルに対応するモデルには何を使っても良い．

### 14.5 交差検証
#### 実際にモデルの性能を未知データで試す
推定されたモデルの未知のデータに対する説明能力を計測するために，データをモデル推定に用いる訓練データと性能をテストするテストデータに分ける．
単純に訓練データとテストデータの2つに分けて性能評価を行うことをホールドアウト検証(hold-out validation)という．
しかし，訓練データが減ってしまうデメリットがある．
そこで，交差検証(cross-validation)という方法がよく使用される．

K-分割交差検証(K-fold cross-validation): 全体のデータをK個のブロックに分割し，K-1個で学習し，残り1個でテストする．
これをK通り全てで行い，性能の平均値を取って最終的なモデルの性能とする．

leave-one-out交差検証(LOOCV): さらに分割の個数を最大限まで増やし，テスト用のデータを1サンプルだけ残して交差検証する方法．

#### リーケージには気を付ける
リーケージ(leakage): 訓練データにテストデータの何らかの情報が残っている場合(本来は見えないテストデータの情報が訓練時に見えている)，モデルの性能が不当に高くなること．
時系列データでは特に気をつける．
データに対する前処理を全体に行う場合も情報がリークすることがあるので，分割した後に行う．
基本的な考え方として，今手元に持っていないデータを将来取得することができたとして，それに対する性能評価を後で行う時に踏む手続きと同じになっているかをチェックする．

#### モデルの信憑性と未知のデータ
数理モデルの推定では，あくまでも仮定したモデルの中で訓練データに一番近いものを選んでいるだけ．
あるデータを説明することができるモデルが，さらに別の新しいデータをたまたまよく説明できることは考えにくい，という考えの元交差検証などが行われている．


---
### 第14章のまとめ
- 「いいモデル」とは，目的を達成するのに役立つモデルのことである
- 目的に応じて，評価する観点・指標が異なる
- テスト用のデータが準備できない時には，情報量基準が便利
- テスト用のデータを準備できる時は，ホールドアウト・交差検証を行う

---
---
## 第四部のまとめ
実際に数理モデルを構築するための各ステップについて，考えるべきポイント，具体的な方法論について解説した．
一般論としての内容を個別の文脈と照らし合わせて反芻するとより深い理解につながる．


---
---
---
# あとがき
数理モデルの内容を入門的な説明で俯瞰的に解説した．
今後の数理モデルに関する学習が見通しよくなる．
様々な情報を持っていないと，その手法でないと理由が分かったり，他の解決方法を検討することができる．
個別の細かい方法は別の参考書を読む．


