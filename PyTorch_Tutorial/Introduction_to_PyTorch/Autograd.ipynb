{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7cd6b7-ba1d-4270-ae75-686f6cf37929",
   "metadata": {},
   "source": [
    "# Automatic differentiation with `torch.autograd`\n",
    "When training neural networks, the most frequently used alghrithm is **back propagation**.\n",
    "In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss  function with respect to the given paramter.\n",
    "\n",
    "To compute those gradients, PyTorch has built-in differentiation engine called `torch.autograd`.\n",
    "It supports automatic computation of gradient for any computational graph.\n",
    "\n",
    "Consider the simplest one-layer neural network, with input `x`, parameters `w` and `b`, and some loss function.\n",
    "It can be defined in PyTorch in the following manner:\n",
    "\n",
    "ニューラルネットワークの訓練の時に，最もよく使うアルゴリズムは**誤差逆伝播法**である．\n",
    "このアルゴリズムにおいては，与えられたパラメータに対する損失関数の勾配に従ってパラメータは調整される．\n",
    "\n",
    "これらの勾配を計算するために，PyTorchには`torch.autograd`と言われる組み込みの微分エンジンがある．\n",
    "それによって，あらゆる計算グラフに対しても，自動的に勾配を計算することができる．\n",
    "\n",
    "最も単純なニューラルネットワーク，入力が`x`，パラメータが`w`，`b`，ある損失関数の1層のニューラルネットワークを考える．\n",
    "PyTorchでは，次の文法に従って定義される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4809b106-87f8-4d8f-809a-d447d6670701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.8.1\n",
      "x: tensor([1., 1., 1., 1., 1.])\n",
      "y: tensor([0., 0., 0.])\n",
      "w: tensor([[-0.1022,  0.3488, -0.0435],\n",
      "        [ 0.6768,  0.7786,  0.0043],\n",
      "        [-1.3720,  0.7665, -1.8001],\n",
      "        [-0.1794, -1.7732,  0.8966],\n",
      "        [-0.8091,  0.1308,  0.9481]], requires_grad=True)\n",
      "b: tensor([-1.3612, -2.4295, -0.8523], requires_grad=True)\n",
      "z: tensor([-3.1471, -2.1780, -0.8470], grad_fn=<AddBackward0>)\n",
      "loss: 0.16871707141399384\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f'torch version: {torch.__version__}')\n",
    "\n",
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "print(f'x: {x}')\n",
    "print(f'y: {y}')\n",
    "print(f'w: {w}')\n",
    "print(f'b: {b}')\n",
    "print(f'z: {z}')\n",
    "print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab2495a-2065-4697-ba3a-70333fa5a0d4",
   "metadata": {},
   "source": [
    "## Tensors, Functions and Computational graph\n",
    "This code defines the following computational graph:\n",
    "![computational graph](graph/autograd_computational_graph.png)\n",
    "\n",
    "In this network, `w` and `b` are parameters, which we need to optimize.\n",
    "Thus, we need to be abel to compute the gradients of loss function with respect to those variables.\n",
    "In order to do that, we set the `requires_grad` property of those tensors.\n",
    "\n",
    "このコードは次の計算グラフで定義される．\n",
    "\n",
    "このネットワークにおいて，`w`と`b`がパラメータで，最適化が必要である．\n",
    "それゆえ，これらの変数に対する損失関数の勾配を計算することが必要である．\n",
    "そのためにも，これらのtensorの`requires_grad`プロパティをセットする必要がある．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048764f-ecac-443e-bf67-5251c5a583c8",
   "metadata": {},
   "source": [
    "#### Note\n",
    "You can set the value of `requires_grad` when creating a tensor, or later by using `x.required_grad_(True)` method.\n",
    "\n",
    " tensorを定義した時に，`requires_grad`をセットする．もしくは後で後から`x.required_gard_(True)`メソッドを用いる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f113f04-463c-4aa1-8ef7-6c9596a49ce4",
   "metadata": {},
   "source": [
    "A function that we apply to tensors to construct computational graph is in fact an object of class `Function`.\n",
    "This object knows how to compute the function in the forward direction, and also how to compute its derivative during the backward propagation step.\n",
    "A reference to the backward propagation function is stored in `grad_fn` property of a tensor.\n",
    "You can find more information of `Function` [in the documentation](https://pytorch.org/docs/stable/autograd.html#function).\n",
    "\n",
    "計算グラフを作成するためにtensorに適用する関数は，実際に`Function`クラスのオブジェクトである．\n",
    "このオブジェクトは順方向の関数を計算する方法と，逆伝播における微分の計算方法を備えている．\n",
    "逆伝播関数への参照はtensorの`grad_fn`プロパティに保存されている．\n",
    "詳細を知りたい場合は，`Function`を見ること．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f073616-55e6-4502-b98d-6bb2fbfe4f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z =  <AddBackward0 object at 0x105efad30>\n",
      "Gradient function for loss =  <BinaryCrossEntropyWithLogitsBackward object at 0x105efab80>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z = ', z.grad_fn)\n",
    "print('Gradient function for loss = ', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc8315-2363-4eaa-b6fb-141ba2f54402",
   "metadata": {},
   "source": [
    "## Computing Gradients\n",
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function iwth respect to parameters, namely, we need $\\dfrac {\\partial loss}{\\partial w}$ and $\\dfrac{\\partial loss}{\\partial b}$ undersome fixed values of `x` and `y`.\n",
    "To compute those derivatives, we call `loss.backward()`, and then retrieve the values from `w.grad` and `b.grad`.\n",
    "\n",
    "ニューラルネットワークのパラメータの係数を最適化するために，パラメータに対する損失関数の微分を計算する必要があり，\n",
    "つまり，ある`x`と`y`の固定値に元での$\\dfarc{\\partial y_}{\\partial x_}$が必要である．\n",
    "これら微分を計算するために，`loss.backward()`を呼び出し，その後，`w.grad`と`b.grad`から値を得ることができる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e4f7b9-8a97-46f5-8451-d2b0a423a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "tensor([[0.0137, 0.0339, 0.1000],\n",
      "        [0.0137, 0.0339, 0.1000],\n",
      "        [0.0137, 0.0339, 0.1000],\n",
      "        [0.0137, 0.0339, 0.1000],\n",
      "        [0.0137, 0.0339, 0.1000]])\n",
      "tensor([0.0137, 0.0339, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c4f90-f5b1-40b0-9d05-7baf1e7f25c8",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "- We can only obtain the `grad` properties for the leaf nodes of the computational graph, which have `requires_grad` property set to `True`. For all other nodes in our graph, gradients will not be available.\n",
    "- We can only perform gradient calculations using `backward` once on a given graph, for performance reasons. If we need to do several `backward` calls on the same graph, we need to pass `retain_graph=True` to the `backward` call.\n",
    "\n",
    "- `requires_grad`プロパティを`True`にセットした計算グラフのノードの`grad`プロパティのみを得ることができる．計算グラフの他のノードは勾配を使用できない．\n",
    "- パフォーマンス上の理由から，与えられたグラフに対する`backward`を用いた勾配の計算は一度だけである．もし同じ計算グラフに対して何回か`backward`を実行したいなら，`backward`の呼び出しに`retain_graph=True`を渡す必要がある．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d732fce-206c-4251-8fa8-e6d11a0bea1f",
   "metadata": {},
   "source": [
    "## Disabling Gradient Tracking\n",
    "By default, all tensors with `requires_grad=True` are tracking their computational history and support gradient computation.\n",
    "However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network.\n",
    "We can stop tracking computations by surrounding our computation code with `torch.no_grad()` block:\n",
    "\n",
    "デフォルトでは，`requires_grad=True`を持った全てのtensorは計算過程を追跡されていて，勾配計算をサポートしている．\n",
    "しかしながら，そのようなことが必要ない場合，例えばモデルを訓練して同じ入力データに適用したい場合，つまりネットワークの順伝播しか必要ない場合がある．\n",
    "`troch.no_grad()`ブロック内に計算コードを書くことで，計算の追跡を止めることが可能である．\n",
    "\n",
    "補足；\n",
    "テストデータやバリデーションデータに対しても使用する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4267699c-5c4b-42c6-a2ee-9eaf53946015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b1ad1-91a9-4979-adf8-2e0e564c328c",
   "metadata": {},
   "source": [
    "Another way to achive the same result is to use the `detach()` method on the tensor:\n",
    "\n",
    "別の方法として，tensorに対して，`detach()`メソッドを用いる方法もある．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d4537b0-8d81-42f2-accc-7cead6ef0176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z.requires_grad)\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e0140-2bbb-41c4-bfd7-30c640075812",
   "metadata": {},
   "source": [
    "There are reasons you might want to disable gradient tracking:\n",
    "\n",
    "- To mark some parameters in your neural network at **frozen parameters**. This is a very common scenario for [finetuning a prettained network](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)\n",
    "- To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.\n",
    "\n",
    "勾配追跡を無効にする理由はおそらく以下の通りである．\n",
    "\n",
    "- ニューラルネットワークにといて，一部のパラメータを**凍結パラメータ**でマークしたい．これは事前に訓練されたネットワークを微調整する一般的な方法である．\n",
    "- 順伝播を計算する時に計算速度を上げたい．というのも，勾配を追跡しないtensorの計算は非常に効率的である．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7a20d-e358-4a87-822f-0f7b49baca8d",
   "metadata": {},
   "source": [
    "## More on Computational Graphs\n",
    "Conceptually, autograd keeps a record of data (tensors) and all executed operations (along with the resulting ner tensors) in directed acyclic graph (DAG) consisting of [Functions](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) objects.\n",
    "In this DAG, leaves are the inout tensors, roots are the output tensors.\n",
    "By tracing this graph from roots to leaves, you automatically compute the gradients using the chain rule.\n",
    "\n",
    "In a forward pass, autograd does two things simultaneously:\n",
    "\n",
    "- run the requestd operation to compute a resulting tensor\n",
    "- maintain the operation's _graient function_ in the DAG.\n",
    "\n",
    "The backward pass kicks off when `.backward()` is called on the DAG root.\n",
    "`autograd` then:\n",
    "\n",
    "- computes the gradients from each `.grad_fn`\n",
    "- accumulates them in the respective tensor's `.grad` attribute\n",
    "- using the cahin rule, propagates all the way to the leaf tensors.\n",
    "\n",
    "概念的には，autogradはデータ(tensor)と実行された全ての操作(結果の新しいtensorと共に)を，関数オブジェクトで構成される有向非巡回グラフ(DAG)に保持する．\n",
    "このDAG内には，葉は入力tensor，根は出力tensorである．\n",
    "このグラフを根から葉までトレースすることにより，連鎖律をおちいて自動的に勾配を計算することができる．\n",
    "\n",
    "順伝播において，autogradは同時に二つのことを行なっている．\n",
    "\n",
    "- 要求された操作を実行して，結果のtensorを計算する\n",
    "- DAG内で操作の_gradient function_を維持する\n",
    "\n",
    "`.backward()`がDAGの値で呼ばれると，逆伝播が始まる．`autograd`は\n",
    "\n",
    "- それぞれの`.grad_fn`から勾配を計算する\n",
    "- それらの値をそれぞれのtensorの`.grad`アトリビュートに累積していく\n",
    "- 連鎖律を用いて，葉の方向へ全て伝播していく"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb9231-54de-4864-8c27-155209ccdfd5",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "**DAGs are dynamic in PyTorch**\n",
    "An important thing to note is that the graph is recreated from scatch; \n",
    "after each `.backward()` call, autograd starts populating a new graph.\n",
    "This is exactly what allows you to use control flow statements in your model;\n",
    "you can cahnge the shape, size and operations at every iteration if needed.\n",
    "\n",
    "**PyTorchにおけるDAGのダイナミクス**\n",
    "\n",
    "注意すべき重要な点は，グラフが最初から再生成されることである．\n",
    "`.backward()`が呼ばれ，autogradは新しいグラフの作成を開始する．\n",
    "これはまさに，モデルで制御フローステートメントを使用できるようにするものである．\n",
    "必要に応じてイテレーション毎に，形やサイズ，操作を変更することが可能である．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb976d-8974-47c5-9277-8a91eb0df89e",
   "metadata": {},
   "source": [
    "## Optional Reading: Tensor Gradients and Jacobian Products\n",
    "In many cases, we have a scalar loss function, and we need to compute the gradient with respect to some parameters.\n",
    "However, there are cases when the output function is arbitrary tensor.\n",
    "In this case, PyTorch allows you to compute so-called **Jacobian product**, and not the actual gradient.\n",
    "\n",
    "For a vector function $\\vec{y} = f(\\vec{x})$, where $\\vec{x} = \\langle x_1, ..., x_n \\rangle$ and \n",
    "$\\vec{y} = \\langle y_1, ... y_m \\rangle$, agradient of $\\vec{y}$ with respect to $\\vec{x}$ is given by **Jacobian matrix**:\n",
    "$$\n",
    "J = \\left(\n",
    "    \\begin{array}{ccc}\n",
    "        \\dfrac{\\partial y_1}{\\partial x_1} & \\ldots & \\dfrac{\\partial y_1}{\\partial x_n} \\\\\n",
    "        \\vdots & \\ddots & \\vdots \\\\\n",
    "        \\dfrac{\\partial y_m}{\\partial x_n} & \\ldots & \\dfrac{\\partial y_m}{\\partial x_n} \\\\\n",
    "    \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Instead of computing the Jacobian matrix itself, PyTorch allows you to compute **Jacobian Product** $v^T \\cdot J$ for a given input vector $v = (v_1 ... v_m)$.\n",
    "This is achieved by calling `backward` with $v$ as an argument.\n",
    "The size of $v$ should be the same as the size of the original tensor, with respect to which we want to compute the product:\n",
    "\n",
    "\n",
    "多くのケースでは，スカラー損失関数があり，いくつかのパラメータに関して勾配を計算する必要がある．\n",
    "しかしながら，出力関数が任意のtensorである場合もある．\n",
    "この場合では，PyTorchを使用すると，実際の勾配ではなく**ヤコビアン積**を計算をする．\n",
    "\n",
    "ベクトル関数$\\vec{y} = f(\\vec{x})$に対して，$\\vec{x} = \\langle x_1, ..., x_n \\rangle$， \n",
    "$\\vec{y} = \\langle y_1, ... y_m \\rangle$とすると，$\\vec{x}$に対する$\\vec{y}$は上記の**ヤコビアン行列**で与えられる．\n",
    "ヤコビアン行列の代わりに，PyTorchではヤコビアン積$v^T \\cdot J$を計算する．\n",
    "$v = (v_1 ... v_m)$で与えられる．\n",
    "これは$v$を引数として，`backward`を呼び出すことで可能である．\n",
    "$v$のサイズは，積を計算する元のtensorと同じサイズでなければならない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c055cb29-05c3-47c0-9439-dc086e2bdddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp\n",
      " tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]], requires_grad=True)\n",
      "out\n",
      " tensor([[4., 1., 1., 1., 1.],\n",
      "        [1., 4., 1., 1., 1.],\n",
      "        [1., 1., 4., 1., 1.],\n",
      "        [1., 1., 1., 4., 1.],\n",
      "        [1., 1., 1., 1., 4.]], grad_fn=<PowBackward0>)\n",
      "\n",
      "First call\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "\n",
      "Second call\n",
      " tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Cakk after zeroing gradients\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)\n",
    "print('inp\\n', inp)\n",
    "print('out\\n', out)\n",
    "\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print('\\nFirst call\\n', inp.grad)\n",
    "\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print('\\nSecond call\\n', inp.grad)\n",
    "\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print('\\nCakk after zeroing gradients\\n', inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2278a4-e484-42ab-96a3-6ee4e546dddb",
   "metadata": {},
   "source": [
    "Notice that when we call `backward` for the second time with the same argument, the value of the gradient is different.\n",
    "This happens because when doing `backward` propagation, PyTorch **accumulates the gradients**, i.e. the value of computed gradients is added to the `grad` property of all leaf nodes of computational graph.\n",
    "If you want to compute the proper gradients, you need to zero out the `grad` propetry before.\n",
    "In real-life training an _optimizer_ helps us to do this.\n",
    "\n",
    "同じ引数を使用して2回目に`backward`を使用すると，勾配が異なることに注意が必要である．\n",
    "これは，`backward`伝播をした時に，PyTorchは勾配を累積する，つまり，計算した勾配の値は計算グラフの全てのノードの`grad`プロパティに加算されていくためである．\n",
    "適切な勾配を計算するためには，逆伝播する前に，`grad`プロパティを0にする必要がある．\n",
    "実際のトレーニングでは_optimizer_がkろえを助けてくれる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8261d-b521-45cf-a52d-364e9fc17af4",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "Previously we were calling `backward()` function without parameters.\n",
    "This is essentially equivalent to calling `backward(torch.tensor(1.0))`, which is a useful way to compute the gradients in case of a scaller-valued function, such as loss during neural network training.\n",
    "\n",
    "以前はパラメータなしで`backward()`関数を呼び出した．\n",
    "これは`backward(torch.tensor(1.0))`を呼び出したことと同じであり，ニューラルネットワーク訓練中の損失など，スカラー値関数の勾配を計算する時に便利である．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ddcaf-4686-4f2a-bd47-db6107955333",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "- [Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
