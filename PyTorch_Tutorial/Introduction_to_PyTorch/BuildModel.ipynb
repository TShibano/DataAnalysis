{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5fb5a7-51c3-4ee8-86bc-6ea0a6293068",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n",
    "Neural networks compise of layers/modules that perform operations on data.\n",
    "The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to build your own neural network.\n",
    "Every module in PyTorch subclasses the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "A neural network is a module itself that consists of other modules (layers).\n",
    "This nested structure allows for building and managing complex architectures easily.\n",
    "\n",
    "In the following sections, we'll build a neural network to classify images in the FashionMNIST dataset.\n",
    "\n",
    "ニューラルネットワークはデータに対して実行するlayerとmoduleから構成されている．\n",
    "torch.nn名前空間は，ニューラルネットワークを組み立てるのに必要なブロックが全てビルドされている．\n",
    "PyTorchのすべてのモジュールはnn.Moduleをサブクラス化する．\n",
    "ニューラルネットワークはそれ自身が他のモジュール(レイヤー)を含んだモジュールになっている．\n",
    "このネストされた構造によって，複雑なアーキテクチャを組み立てて管理することが出来る．\n",
    "\n",
    "以下のセクションでは，FashionMNISTデータセットの画像を分類するために，ニューラルネットワークを組み立てる．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefdc3bd-aa4f-4b1f-a23c-57f18ee87271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964ef355-063c-4ac4-ab53-9bd83296e4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa69628-9a4c-47f4-9f93-220a609f7d1c",
   "metadata": {},
   "source": [
    "## Get Device for Training\n",
    "We want to be able to train our model on a hardware accelerator lile the GPU, if it is available.\n",
    "Let's check to see if [torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) is available, else we continue to use the CPU.\n",
    "\n",
    "もし可能なら，モデル訓練にはGPUのようなハードウェアアクセラレーションを使いたい．\n",
    "torch.cudaが使えるかチェックし，使えないなら引き続きCPUを使う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f57412-50b5-4a82-8e77-a885910e5372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device.'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e72ba-0416-401f-8d09-2bb6ae5f820c",
   "metadata": {},
   "source": [
    "## Define the Class\n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`.\n",
    "Every `nn.Module` subclass implements the operations on input data in the `forward` method.\n",
    "\n",
    "`nn.Module`をサブクラス化して，ニューラルネットワークを定義し，`__init__`内でニューラルネットワークのレイヤーを初期化する．\n",
    "全ての`nn.Module`サブクラスは`forward`メソッド内で，入力データに対する操作を実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9966c3c-86fe-4a3f-9b36-93c8275c6b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_rel_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_rel_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0895519-1d77-4be6-ae02-0eb98ad68325",
   "metadata": {},
   "source": [
    "We create an instance of `NeuralNetwork`, and move it to the `device`, and print its structure.\n",
    "\n",
    "`NeuralNetwork`のインスタンスを作り，`device`にモデルを移し，構造を表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12eeeacb-b0c1-4f2e-a3dc-cbfff00e1163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_rel_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae44ea-472e-4ae7-8889-e0ba8d8b5fa1",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data.\n",
    "This executes the model's `forward`, along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n",
    "Do not call `mdoel.forward()` directory!\n",
    "\n",
    "Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each calss.\n",
    "We get the prediction probabilities by passing it through an instance of the `nn.Softmax` module.\n",
    "\n",
    "モデルを使うために，入力データを渡す．\n",
    "これにより，いくつかのバックグラウンド操作とともに，モデルの`forward`が実行される．\n",
    "直接`model.forward()`をしてはいけない．\n",
    "\n",
    "入力してモデルを呼び出すと，それぞれのクラスに対した生の予測値をもった10次元tensorが返される．\n",
    "`nn.Softmax`モジュールのインスタンスを通して，予測確率を計算する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "056044cf-225c-4089-983c-4a58f286d5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([9])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "# print(f'X: {X}')\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f'Predicted class: {y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd3e504-9ac9-4eff-a36b-067e5c8b6146",
   "metadata": {},
   "source": [
    "## Model Layers\n",
    "Let's break down the layers in the FashionMNIST model.\n",
    "To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see waht happends to it as we pass it through the network.\n",
    "\n",
    "FashionMNISTモデルのレイヤーを分解してみよう．\n",
    "そのために，28x28サイズの3つの画像を持ったミニバッチサンプルを作り，ネットワークに渡した時に何が起こるかを見る．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ecb227e-d9af-4fec-b448-dd8f8c1918d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8128, 0.3292, 0.2968,  ..., 0.8809, 0.1153, 0.7465],\n",
      "         [0.0501, 0.6921, 0.8031,  ..., 0.5209, 0.0278, 0.4134],\n",
      "         [0.2398, 0.5149, 0.4355,  ..., 0.7992, 0.4206, 0.2928],\n",
      "         ...,\n",
      "         [0.8844, 0.0121, 0.2719,  ..., 0.5298, 0.8784, 0.7293],\n",
      "         [0.2733, 0.8275, 0.6309,  ..., 0.9957, 0.6732, 0.2083],\n",
      "         [0.4098, 0.7956, 0.5423,  ..., 0.3188, 0.1732, 0.5116]],\n",
      "\n",
      "        [[0.4582, 0.1576, 0.0280,  ..., 0.6725, 0.4204, 0.0593],\n",
      "         [0.4355, 0.5572, 0.5439,  ..., 0.1069, 0.3926, 0.6804],\n",
      "         [0.5919, 0.2988, 0.7558,  ..., 0.4591, 0.6902, 0.4303],\n",
      "         ...,\n",
      "         [0.8699, 0.2252, 0.4551,  ..., 0.6674, 0.8577, 0.9826],\n",
      "         [0.0012, 0.3672, 0.8195,  ..., 0.0352, 0.1489, 0.7793],\n",
      "         [0.9581, 0.1901, 0.6193,  ..., 0.3620, 0.2060, 0.4264]],\n",
      "\n",
      "        [[0.7948, 0.3452, 0.5529,  ..., 0.6459, 0.9062, 0.9466],\n",
      "         [0.2956, 0.3752, 0.6322,  ..., 0.8081, 0.8761, 0.5511],\n",
      "         [0.5486, 0.7784, 0.5887,  ..., 0.5057, 0.9832, 0.6913],\n",
      "         ...,\n",
      "         [0.2052, 0.0853, 0.9829,  ..., 0.9118, 0.8582, 0.3117],\n",
      "         [0.2125, 0.4610, 0.0419,  ..., 0.0932, 0.9467, 0.3552],\n",
      "         [0.1180, 0.7308, 0.6400,  ..., 0.3089, 0.2237, 0.8330]]])\n",
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9df847-03b4-41c8-a6dc-08783b72de22",
   "metadata": {},
   "source": [
    "### nn.Flatten\n",
    "We initialize the [nn.Flaten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer to convert wach 2D 28x28 image into a contiguous array of 784 pixec values (the minibatch dimension (at dim=0) is maintained).\n",
    "\n",
    "nn.Flatenレイヤーを初期化して，2Dの28x28画像を784ピクセル値の連続した配列に変換する(ミニバッチの次元(=0)は維持される)．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3829cc8b-d31c-4d6c-b33d-74d953157fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8128, 0.3292, 0.2968,  ..., 0.3188, 0.1732, 0.5116],\n",
      "        [0.4582, 0.1576, 0.0280,  ..., 0.3620, 0.2060, 0.4264],\n",
      "        [0.7948, 0.3452, 0.5529,  ..., 0.3089, 0.2237, 0.8330]])\n",
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a6d4e-5d4b-41c3-ae7a-166014b55735",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using its stored weights and biases.\n",
    "\n",
    "linear layerはモジュールで，保存された重みとバイアスを用いて入力データを線形変換する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7a78713-33f7-4089-b59a-b75d501810b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1770, -1.1567,  0.0379, -0.3887,  0.0970,  0.3951,  0.1034, -0.7620,\n",
      "          0.2434,  0.2521, -0.2528,  0.2172,  0.5640,  0.2983,  0.2867, -0.5163,\n",
      "         -0.1014,  0.2721,  0.4994, -0.3706],\n",
      "        [-0.1621, -1.0222,  0.1181, -0.5159, -0.2740,  0.1259,  0.2653, -0.6011,\n",
      "          0.1555,  0.0501, -0.2117,  0.0173,  0.8495,  0.1052,  0.3524,  0.0571,\n",
      "         -0.2359,  0.3415,  0.3847, -0.2149],\n",
      "        [-0.1070, -0.6417,  0.2445, -0.4113, -0.1074,  0.2096,  0.2330, -0.6409,\n",
      "         -0.0306,  0.1443, -0.0565,  0.1530,  0.5799,  0.0259,  0.6064, -0.6184,\n",
      "         -0.1025,  0.1217,  0.2010, -0.1655]], grad_fn=<AddmmBackward>)\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c6d6b0-c4eb-4012-801a-dc7d7067650f",
   "metadata": {},
   "source": [
    "### nn.ReLU\n",
    "Non-linear activations are what create the complex mappings between the model's inputs and outputs.\n",
    "They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
    "\n",
    "In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there's other activations to introduce non-linearity in your model.\n",
    "\n",
    "非線形活性化関数はモデルの入力と出力間の関係性を複雑にするものである．\n",
    "非線形活性化関数を，線形変換後に非線形性を導入することで，ニューラルネットワークが様々な現象を学習するのを助ける．\n",
    "\n",
    "このモデルでは，nn.ReLUを線形層の間で用いるが，他の非線形性を導入する活性化関数も用いることが出来る．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20159ea5-926d-441e-8c5f-4c22b9f2b451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLu: tensor([[ 0.1770, -1.1567,  0.0379, -0.3887,  0.0970,  0.3951,  0.1034, -0.7620,\n",
      "          0.2434,  0.2521, -0.2528,  0.2172,  0.5640,  0.2983,  0.2867, -0.5163,\n",
      "         -0.1014,  0.2721,  0.4994, -0.3706],\n",
      "        [-0.1621, -1.0222,  0.1181, -0.5159, -0.2740,  0.1259,  0.2653, -0.6011,\n",
      "          0.1555,  0.0501, -0.2117,  0.0173,  0.8495,  0.1052,  0.3524,  0.0571,\n",
      "         -0.2359,  0.3415,  0.3847, -0.2149],\n",
      "        [-0.1070, -0.6417,  0.2445, -0.4113, -0.1074,  0.2096,  0.2330, -0.6409,\n",
      "         -0.0306,  0.1443, -0.0565,  0.1530,  0.5799,  0.0259,  0.6064, -0.6184,\n",
      "         -0.1025,  0.1217,  0.2010, -0.1655]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.1770, 0.0000, 0.0379, 0.0000, 0.0970, 0.3951, 0.1034, 0.0000, 0.2434,\n",
      "         0.2521, 0.0000, 0.2172, 0.5640, 0.2983, 0.2867, 0.0000, 0.0000, 0.2721,\n",
      "         0.4994, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1181, 0.0000, 0.0000, 0.1259, 0.2653, 0.0000, 0.1555,\n",
      "         0.0501, 0.0000, 0.0173, 0.8495, 0.1052, 0.3524, 0.0571, 0.0000, 0.3415,\n",
      "         0.3847, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2445, 0.0000, 0.0000, 0.2096, 0.2330, 0.0000, 0.0000,\n",
      "         0.1443, 0.0000, 0.1530, 0.5799, 0.0259, 0.6064, 0.0000, 0.0000, 0.1217,\n",
      "         0.2010, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Before ReLu: {hidden1}\\n\\n')\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f'After ReLU: {hidden1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe0584a-2eaa-48b2-b946-ae9c924fc9a0",
   "metadata": {},
   "source": [
    "### nn.Sequentioal\n",
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an orderd container of modules.\n",
    "The data is passed through all the modules in the same order as defined.\n",
    "You can use sequential containers to put togegher a quick network lile `seq_modelues`.\n",
    "\n",
    "nn.Sequentialはモジュールの順序づけられたコンテナである．\n",
    "データは定義された順番に全てのモジュールを通る．\n",
    "シーケンシャルコンテナを使用して，`seq_modules`のようなクイックネットワークをまとめることができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20b1df0a-b2e9-4191-ab86-b26d5603ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.9027e-01,  2.3255e-01, -2.9550e-04,  5.4221e-01,  7.0530e-02,\n",
      "         -2.8926e-01, -1.2192e-01,  2.0138e-01, -3.1618e-01, -4.0473e-01],\n",
      "        [ 2.7024e-01,  1.7551e-01,  7.9698e-02,  3.3475e-01,  1.2269e-02,\n",
      "         -2.8137e-01, -1.8714e-01,  8.8177e-02, -2.9120e-01, -1.9627e-01],\n",
      "        [ 3.2363e-01,  2.0389e-01,  1.7663e-02,  4.5542e-01,  8.2976e-02,\n",
      "         -2.5510e-01, -1.4627e-01,  1.6944e-01, -3.2762e-01, -2.6919e-01]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten, \n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeee1a2-c325-49f5-aa02-d357aa43d615",
   "metadata": {},
   "source": [
    "### nn.Softmax\n",
    "The last linear layer of the neural network returns logits - raw values in \\[-$\\infty$, $\\infty$\\] - which are passed to the [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module.\n",
    "The logtis are scaled to vlaues \\[0, 1\\] representing the model's predicted probabilities for each class. \n",
    "`dim` parameter indicates the dimension along which the values must sum to 1.\n",
    "\n",
    "ニューラルネットワークの最後の線形層はロジット\\[-$\\infty$, $\\infty$\\]を返し，これをnn.Softmaxモジュールに渡す．\n",
    "ロジットはそれぞれのクラスを予測する確率を表現する\\[0, 1\\]にスケーリングされる．\n",
    "`dim`パラメータは合計が1になる次元を指す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ba2066d-f84e-44b3-bf73-8fbbc5a184d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1256, 0.1185, 0.0939, 0.1616, 0.1008, 0.0703, 0.0832, 0.1149, 0.0685,\n",
      "         0.0627],\n",
      "        [0.1280, 0.1164, 0.1058, 0.1365, 0.0989, 0.0737, 0.0810, 0.1067, 0.0730,\n",
      "         0.0802],\n",
      "        [0.1304, 0.1157, 0.0960, 0.1488, 0.1025, 0.0731, 0.0815, 0.1118, 0.0680,\n",
      "         0.0721]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "print(pred_probab)\n",
    "\n",
    "# predicted class\n",
    "pred_class = pred_probab.argmax(dim=1)\n",
    "print(pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521ca53-54c4-4326-adc6-aacb3e2ce245",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training.\n",
    "Subclassing `nn.Module` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model's `parameters()` or `named_parameters()` methods.\n",
    "\n",
    "In this example, we iterate over each parameter, and print its size and a preview of its values.\n",
    "\n",
    "ニューラルネットワークの多くのレイヤーはパラメータ化されている．\n",
    "つまり，訓練の間に最適化された重みとバイアスが関連づけられている．\n",
    "`nn.Module`をサブクラス化することで，モデル内で定義された全てのフィールドを自動的に追跡し，モデルが持つ`parameters()`や`names_parameters()`メソッドを用いることで全てのパラメータにアクセスすることができる．\n",
    "\n",
    "この例では，全てのパラメータを反復処理し，サイズと値を表示する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddd37163-004d-4fcb-bcc7-865a723c6548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moel structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_rel_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_rel_stack.0.weight | Size: torch.Size([512, 784]) | Values: tensor([[ 0.0223,  0.0096, -0.0141,  ..., -0.0350, -0.0044, -0.0019],\n",
      "        [ 0.0065,  0.0278, -0.0346,  ..., -0.0278, -0.0290, -0.0198]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_rel_stack.0.bias | Size: torch.Size([512]) | Values: tensor([ 0.0142, -0.0274], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_rel_stack.2.weight | Size: torch.Size([512, 512]) | Values: tensor([[ 0.0336, -0.0434, -0.0097,  ..., -0.0005,  0.0077, -0.0223],\n",
      "        [-0.0033,  0.0042, -0.0266,  ..., -0.0021,  0.0338, -0.0058]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_rel_stack.2.bias | Size: torch.Size([512]) | Values: tensor([0.0290, 0.0006], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_rel_stack.4.weight | Size: torch.Size([10, 512]) | Values: tensor([[-0.0183,  0.0359, -0.0411,  ...,  0.0229,  0.0250, -0.0313],\n",
      "        [ 0.0324, -0.0047,  0.0294,  ..., -0.0225, -0.0215,  0.0297]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_rel_stack.4.bias | Size: torch.Size([10]) | Values: tensor([-0.0226, -0.0109], grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Moel structure: ', model, '\\n\\n')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8470e4cb-aedc-4335-8838-0e6fb622e133",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "- [torch.nn API](https://pytorch.org/docs/stable/nn.htmlj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
